{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Cancer detection with deep learning\n",
    "\n",
    "The aim of this project is to create and train a deep neural network to detect cancer on images, with high accuracy. The GitHub repository for the project is the following: https://github.com/mohosb/cancer_detection\n",
    "\n",
    "The data is from the Kaggle competition \"Histopathologic Cancer Detection\" (https://www.kaggle.com/competitions/histopathologic-cancer-detection/overview).\n",
    "\n",
    "In this project I will use PyTorch, a popular deep learning library for Python, and also a package called \"nn_utils\", that contains lots of useful functions. This package is writen and developed entirely by me. (https://github.com/szegedai/nn_utils)\n",
    "\n",
    "## Data preprocessing and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision.transforms.functional import normalize as standardize\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from nn_utils.misc import split_dataset, create_data_loaders, RollingStatistics\n",
    "from nn_utils.models import count_parameters\n",
    "from nn_utils.models.resnet_v2 import ResNetV2\n",
    "from nn_utils.training import train_classifier, CLILoggerCallback, LRSchedulerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs(data_path + '/train/0', exist_ok=True)\n",
    "    os.makedirs(data_path + '/train/1', exist_ok=True)\n",
    "\n",
    "    df = pd.read_csv('train_labels.csv', sep=',')\n",
    "    for id, label in zip(df['id'], df['label']):\n",
    "        os.rename(f'{data_path}/train/{id}.tif', f'{data_path}/train/{label}/{id}.tif')\n",
    "    del df\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UnlabeledImageFolder(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform, return_img_id=False):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        self.all_images = sorted(os.listdir(root))\n",
    "        self.return_img_id = return_img_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.all_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(os.path.join(self.root, self.all_images[idx])).convert('RGB')\n",
    "        if self.return_img_id:\n",
    "            return self.transform(image), self.all_images[idx].split('.')[0]\n",
    "        return self.transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_transforms = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.RandomVerticalFlip(),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "test_transforms = torchvision.transforms.ToTensor()\n",
    "train_ds = torchvision.datasets.ImageFolder(data_path + '/train', train_transforms)\n",
    "test_ds = UnlabeledImageFolder(data_path + '/test', test_transforms, return_img_id=True)\n",
    "train_ds, val_ds = split_dataset(train_ds, split=0.1, seed=42)\n",
    "train_loader, val_loader = create_data_loaders(\n",
    "    [train_ds, val_ds],\n",
    "    batch_size=512, shuffle=True,\n",
    "    num_workers=4, pin_memory=True,\n",
    "    multiprocessing_context='spawn', persistent_workers=False\n",
    ")\n",
    "test_loader, = create_data_loaders(\n",
    "    [test_ds], batch_size=512, shuffle=False,\n",
    "    num_workers=0, multiprocessing_context=None, persistent_workers=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EDA\n",
    "\n",
    "Let us look at the data to get an understanding of it. In the following cells, we will see the following:\n",
    "- Length of the training and validations datasets\n",
    "- Some sample images from the training data and their labels\n",
    "- The statistics of the RGB channels over the training data (the models will use this later to standardize the input)\n",
    "- The number of labels for the two classes to see how unbalanced it is"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "fig, ax = plt.subplots(2, 5)\n",
    "for i in range(10):\n",
    "    img, label = train_ds[i]\n",
    "    labels.append(label)\n",
    "    current_ax = ax[i // 5, i % 5]\n",
    "    current_ax.imshow(img.permute(1, 2, 0).numpy())\n",
    "    current_ax.axis('off')\n",
    "print('img size:', list(img.shape))\n",
    "print('labels:', labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Not recommend running this, because it takes a long time even on a fast SSD. On an HDD don't even try it LOL! XD\n",
    "# The result of this code is hardcoded in the next cell. Just use those.\n",
    "stats = RollingStatistics((3, 96 * 96))\n",
    "label_sum = 0\n",
    "for images, labels in train_loader:\n",
    "    stats.update(images.permute(1, 0, 2, 3).flatten(1, -1).numpy())\n",
    "    label_sum += sum(labels)\n",
    "\n",
    "means = stats.mean.tolist()\n",
    "stds = stats.std.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = [0.7024619 , 0.5462504 , 0.69643368]\n",
    "stds = [0.23888772, 0.28208111, 0.21622823]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('label 0 count:', len(train_ds) - label_sum)\n",
    "print('label 1 count:', label_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training and hyperparameter tuning\n",
    "\n",
    "In this section we will set up some model configurations to train different models and try to select the best hyperparameters.\n",
    "The plan for the selection is the following:\n",
    "1) pick some reasonable learning rate and weight decay that we can fix for now\n",
    "2) pick some models with different architectures\n",
    "3) try to increase the width (number of filters per convolution layers) and the depth (number of convolution layers) to see which one gives us better performance\n",
    "4) select the best model architecture and try a few other learning rate and weight decay combinations\n",
    "\n",
    "(In other situations I would do a full hyperparameter search but that would take weeks to run, so I will just pick some arbitrary values of the top of my head.)\n",
    "\n",
    "Our models will use 3x3 convolutional layers (for computational efficiency) paired with 2D batch normalization layers (for stabilizing training performance) for feature extraction and a fully connected final layer for the actual classification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: mps\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float32\n",
    "#dtype = torch.float16  # Use this for faster training and lower memory usage!\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')       # For Nvidia GPUs if cuda is installed\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')        # For Apple silicon (M1, M2)\n",
    "else:\n",
    "    device = torch.device('cpu')        # If nothing else is available use the CPU (not recommended!)\n",
    "print('using:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit(model, base_lr, wd, num_epochs=3, verbose=False):\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=base_lr, weight_decay=wd)\n",
    "    def lr_fn(epoch):\n",
    "        if epoch < 2:  # [1, 2)\n",
    "            return 1.\n",
    "        if epoch < 3:  # [2, 3)\n",
    "            return 1e-1\n",
    "        return 1e-2  # [3, inf)\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_fn)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    callbacks = [\n",
    "        LRSchedulerCallback(scheduler)\n",
    "    ]\n",
    "    if verbose:\n",
    "        callbacks.append(CLILoggerCallback())\n",
    "\n",
    "    final_metrics = train_classifier(\n",
    "        model, loss_fn, optimizer, train_loader, val_loader,\n",
    "        callbacks=callbacks, num_epochs=num_epochs\n",
    "    )\n",
    "    return final_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self, num_classes, depth_factor=5, width_factor=1, means=(0., 0., 0.), stds=(1., 1., 1.)):\n",
    "        super().__init__()\n",
    "\n",
    "        num_channels = [16] + [2 ** (4 + i) * width_factor for i in range(depth_factor)]\n",
    "        self.conv_layers = [\n",
    "            nn.Conv2d(3, num_channels[0], kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(num_channels[0]),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        for i in range(1, depth_factor + 1):\n",
    "            self.conv_layers += [\n",
    "                nn.Conv2d(num_channels[i - 1], num_channels[i], kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(num_channels[i]),\n",
    "                nn.Conv2d(num_channels[i], num_channels[i], kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(num_channels[i]),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "        self.conv_layers = nn.ModuleList(self.conv_layers)\n",
    "        self.classifier = nn.Linear(num_channels[-1], num_classes)\n",
    "\n",
    "        self.means = means\n",
    "        self.stds = stds\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = standardize(x, self.means, self.stds)\n",
    "        for layer in self.conv_layers:\n",
    "            x = layer(x)\n",
    "        x = torch.mean(x, (2, 3))\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following cell, uncomment the model architecture and the configuration, you would like to use."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNetV2(\n",
      "  (head): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  (groups): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation_fn): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation_fn): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (transition): Sequential(\n",
      "          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "          (1): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation_fn): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation_fn): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (transition): Sequential(\n",
      "          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "          (1): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation_fn): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation_fn): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): BasicBlock(\n",
      "        (transition): Sequential(\n",
      "          (0): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "          (1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation_fn): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (1): BasicBlock(\n",
      "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (activation_fn): ReLU(inplace=True)\n",
      "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (activation_fn): ReLU(inplace=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (linear): Linear(in_features=128, out_features=2, bias=True)\n",
      ")\n",
      "number of parameters: 699986\n"
     ]
    }
   ],
   "source": [
    "#model = BasicCNN(2, depth_factor=3, width_factor=1, means=means, stds=stds)\n",
    "#model = BasicCNN(2, depth_factor=3, width_factor=2, means=means, stds=stds)\n",
    "#model = BasicCNN(2, depth_factor=4, width_factor=1, means=means, stds=stds)\n",
    "#model = ResNetV2(2, [2, 2, 2], width_factor=1, means=means, stds=stds)\n",
    "#model = ResNetV2(2, [2, 2, 2], width_factor=2, means=means, stds=stds)\n",
    "model = ResNetV2(2, [2, 2, 2, 2], width_factor=1, means=means, stds=stds)\n",
    "\n",
    "#base_learning_rate, weight_decay = 0.001, 0.0001\n",
    "#base_learning_rate, weight_decay = 0.0005, 0.0001\n",
    "#base_learning_rate, weight_decay = 0.005, 0.0001\n",
    "base_learning_rate, weight_decay = 0.005, 0.0005\n",
    "\n",
    "model.to(device=device, dtype=dtype)\n",
    "print(next(model.modules()))\n",
    "print('number of parameters:', count_parameters(model))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3 epoch:\n",
      "\n",
      "\u001B[1A\u001B[2K  1/387 3.75s - train_loss: 0.6953 train_acc: 0.6133 \n",
      "\u001B[1A\u001B[2K  2/387 4.15s - train_loss: 0.7191 train_acc: 0.6377 \n",
      "\u001B[1A\u001B[2K  3/387 4.53s - train_loss: 0.7645 train_acc: 0.5553 \n",
      "\u001B[1A\u001B[2K  4/387 4.92s - train_loss: 0.7269 train_acc: 0.5796 \n",
      "\u001B[1A\u001B[2K  5/387 5.31s - train_loss: 0.6928 train_acc: 0.6109 \n",
      "\u001B[1A\u001B[2K  6/387 5.71s - train_loss: 0.6582 train_acc: 0.6426 \n",
      "\u001B[1A\u001B[2K  7/387 6.11s - train_loss: 0.6312 train_acc: 0.6610 \n",
      "\u001B[1A\u001B[2K  8/387 6.49s - train_loss: 0.6098 train_acc: 0.6782 \n",
      "\u001B[1A\u001B[2K  9/387 6.87s - train_loss: 0.5941 train_acc: 0.6914 \n",
      "\u001B[1A\u001B[2K  10/387 7.26s - train_loss: 0.5810 train_acc: 0.7006 \n",
      "\u001B[1A\u001B[2K  11/387 7.65s - train_loss: 0.5743 train_acc: 0.7061 \n",
      "\u001B[1A\u001B[2K  12/387 8.03s - train_loss: 0.5676 train_acc: 0.7109 \n",
      "\u001B[1A\u001B[2K  13/387 8.42s - train_loss: 0.5554 train_acc: 0.7207 \n",
      "\u001B[1A\u001B[2K  14/387 8.81s - train_loss: 0.5441 train_acc: 0.7292 \n",
      "\u001B[1A\u001B[2K  15/387 9.19s - train_loss: 0.5355 train_acc: 0.7366 \n",
      "\u001B[1A\u001B[2K  16/387 9.58s - train_loss: 0.5301 train_acc: 0.7416 \n",
      "\u001B[1A\u001B[2K  17/387 9.98s - train_loss: 0.5272 train_acc: 0.7441 \n",
      "\u001B[1A\u001B[2K  18/387 10.37s - train_loss: 0.5208 train_acc: 0.7473 \n",
      "\u001B[1A\u001B[2K  19/387 10.75s - train_loss: 0.5150 train_acc: 0.7514 \n",
      "\u001B[1A\u001B[2K  20/387 11.15s - train_loss: 0.5114 train_acc: 0.7544 \n",
      "\u001B[1A\u001B[2K  21/387 11.54s - train_loss: 0.5074 train_acc: 0.7576 \n",
      "\u001B[1A\u001B[2K  22/387 11.93s - train_loss: 0.5070 train_acc: 0.7574 \n",
      "\u001B[1A\u001B[2K  23/387 12.32s - train_loss: 0.5029 train_acc: 0.7597 \n",
      "\u001B[1A\u001B[2K  24/387 12.70s - train_loss: 0.5009 train_acc: 0.7610 \n",
      "\u001B[1A\u001B[2K  25/387 13.10s - train_loss: 0.4965 train_acc: 0.7634 \n",
      "\u001B[1A\u001B[2K  26/387 13.50s - train_loss: 0.4944 train_acc: 0.7649 \n",
      "\u001B[1A\u001B[2K  27/387 13.89s - train_loss: 0.4911 train_acc: 0.7668 \n",
      "\u001B[1A\u001B[2K  28/387 14.29s - train_loss: 0.4874 train_acc: 0.7695 \n",
      "\u001B[1A\u001B[2K  29/387 14.70s - train_loss: 0.4838 train_acc: 0.7716 \n",
      "\u001B[1A\u001B[2K  30/387 15.08s - train_loss: 0.4795 train_acc: 0.7743 \n",
      "\u001B[1A\u001B[2K  31/387 15.46s - train_loss: 0.4766 train_acc: 0.7759 \n",
      "\u001B[1A\u001B[2K  32/387 15.85s - train_loss: 0.4736 train_acc: 0.7775 \n",
      "\u001B[1A\u001B[2K  33/387 16.24s - train_loss: 0.4705 train_acc: 0.7795 \n",
      "\u001B[1A\u001B[2K  34/387 16.63s - train_loss: 0.4676 train_acc: 0.7815 \n",
      "\u001B[1A\u001B[2K  35/387 17.02s - train_loss: 0.4658 train_acc: 0.7828 \n",
      "\u001B[1A\u001B[2K  36/387 17.41s - train_loss: 0.4626 train_acc: 0.7843 \n",
      "\u001B[1A\u001B[2K  37/387 17.80s - train_loss: 0.4609 train_acc: 0.7855 \n",
      "\u001B[1A\u001B[2K  38/387 18.19s - train_loss: 0.4591 train_acc: 0.7865 \n",
      "\u001B[1A\u001B[2K  39/387 18.57s - train_loss: 0.4577 train_acc: 0.7875 \n",
      "\u001B[1A\u001B[2K  40/387 18.96s - train_loss: 0.4571 train_acc: 0.7883 \n",
      "\u001B[1A\u001B[2K  41/387 19.35s - train_loss: 0.4563 train_acc: 0.7891 \n",
      "\u001B[1A\u001B[2K  42/387 19.74s - train_loss: 0.4549 train_acc: 0.7897 \n",
      "\u001B[1A\u001B[2K  43/387 20.13s - train_loss: 0.4527 train_acc: 0.7910 \n",
      "\u001B[1A\u001B[2K  44/387 20.50s - train_loss: 0.4524 train_acc: 0.7910 \n",
      "\u001B[1A\u001B[2K  45/387 20.88s - train_loss: 0.4512 train_acc: 0.7918 \n",
      "\u001B[1A\u001B[2K  46/387 21.27s - train_loss: 0.4496 train_acc: 0.7928 \n",
      "\u001B[1A\u001B[2K  47/387 21.65s - train_loss: 0.4486 train_acc: 0.7935 \n",
      "\u001B[1A\u001B[2K  48/387 22.03s - train_loss: 0.4471 train_acc: 0.7947 \n",
      "\u001B[1A\u001B[2K  49/387 22.41s - train_loss: 0.4457 train_acc: 0.7955 \n",
      "\u001B[1A\u001B[2K  50/387 22.79s - train_loss: 0.4447 train_acc: 0.7959 \n",
      "\u001B[1A\u001B[2K  51/387 23.18s - train_loss: 0.4434 train_acc: 0.7970 \n",
      "\u001B[1A\u001B[2K  52/387 23.56s - train_loss: 0.4419 train_acc: 0.7979 \n",
      "\u001B[1A\u001B[2K  53/387 23.94s - train_loss: 0.4417 train_acc: 0.7978 \n",
      "\u001B[1A\u001B[2K  54/387 24.32s - train_loss: 0.4402 train_acc: 0.7988 \n",
      "\u001B[1A\u001B[2K  55/387 24.69s - train_loss: 0.4391 train_acc: 0.7993 \n",
      "\u001B[1A\u001B[2K  56/387 25.07s - train_loss: 0.4388 train_acc: 0.7996 \n",
      "\u001B[1A\u001B[2K  57/387 25.47s - train_loss: 0.4384 train_acc: 0.7998 \n",
      "\u001B[1A\u001B[2K  58/387 25.86s - train_loss: 0.4389 train_acc: 0.7993 \n",
      "\u001B[1A\u001B[2K  59/387 26.25s - train_loss: 0.4377 train_acc: 0.7999 \n",
      "\u001B[1A\u001B[2K  60/387 26.63s - train_loss: 0.4363 train_acc: 0.8005 \n",
      "\u001B[1A\u001B[2K  61/387 27.00s - train_loss: 0.4356 train_acc: 0.8006 \n",
      "\u001B[1A\u001B[2K  62/387 27.38s - train_loss: 0.4348 train_acc: 0.8010 \n",
      "\u001B[1A\u001B[2K  63/387 27.76s - train_loss: 0.4351 train_acc: 0.8011 \n",
      "\u001B[1A\u001B[2K  64/387 28.15s - train_loss: 0.4343 train_acc: 0.8016 \n",
      "\u001B[1A\u001B[2K  65/387 28.53s - train_loss: 0.4337 train_acc: 0.8022 \n",
      "\u001B[1A\u001B[2K  66/387 28.90s - train_loss: 0.4325 train_acc: 0.8028 \n",
      "\u001B[1A\u001B[2K  67/387 29.28s - train_loss: 0.4315 train_acc: 0.8035 \n",
      "\u001B[1A\u001B[2K  68/387 29.65s - train_loss: 0.4313 train_acc: 0.8039 \n",
      "\u001B[1A\u001B[2K  69/387 30.03s - train_loss: 0.4307 train_acc: 0.8040 \n",
      "\u001B[1A\u001B[2K  70/387 30.42s - train_loss: 0.4299 train_acc: 0.8041 \n",
      "\u001B[1A\u001B[2K  71/387 30.80s - train_loss: 0.4288 train_acc: 0.8047 \n",
      "\u001B[1A\u001B[2K  72/387 31.17s - train_loss: 0.4284 train_acc: 0.8048 \n",
      "\u001B[1A\u001B[2K  73/387 31.55s - train_loss: 0.4280 train_acc: 0.8051 \n",
      "\u001B[1A\u001B[2K  74/387 31.94s - train_loss: 0.4274 train_acc: 0.8054 \n",
      "\u001B[1A\u001B[2K  75/387 32.32s - train_loss: 0.4269 train_acc: 0.8057 \n",
      "\u001B[1A\u001B[2K  76/387 32.72s - train_loss: 0.4261 train_acc: 0.8060 \n",
      "\u001B[1A\u001B[2K  77/387 33.10s - train_loss: 0.4256 train_acc: 0.8064 \n",
      "\u001B[1A\u001B[2K  78/387 33.48s - train_loss: 0.4256 train_acc: 0.8064 \n",
      "\u001B[1A\u001B[2K  79/387 33.85s - train_loss: 0.4248 train_acc: 0.8068 \n",
      "\u001B[1A\u001B[2K  80/387 34.23s - train_loss: 0.4243 train_acc: 0.8070 \n",
      "\u001B[1A\u001B[2K  81/387 34.61s - train_loss: 0.4238 train_acc: 0.8073 \n",
      "\u001B[1A\u001B[2K  82/387 35.00s - train_loss: 0.4232 train_acc: 0.8074 \n",
      "\u001B[1A\u001B[2K  83/387 35.37s - train_loss: 0.4227 train_acc: 0.8075 \n",
      "\u001B[1A\u001B[2K  84/387 35.74s - train_loss: 0.4216 train_acc: 0.8083 \n",
      "\u001B[1A\u001B[2K  85/387 36.12s - train_loss: 0.4206 train_acc: 0.8089 \n",
      "\u001B[1A\u001B[2K  86/387 36.51s - train_loss: 0.4202 train_acc: 0.8088 \n",
      "\u001B[1A\u001B[2K  87/387 36.89s - train_loss: 0.4195 train_acc: 0.8090 \n",
      "\u001B[1A\u001B[2K  88/387 37.26s - train_loss: 0.4191 train_acc: 0.8092 \n",
      "\u001B[1A\u001B[2K  89/387 37.63s - train_loss: 0.4181 train_acc: 0.8099 \n",
      "\u001B[1A\u001B[2K  90/387 38.01s - train_loss: 0.4173 train_acc: 0.8104 \n",
      "\u001B[1A\u001B[2K  91/387 38.40s - train_loss: 0.4167 train_acc: 0.8106 \n",
      "\u001B[1A\u001B[2K  92/387 38.79s - train_loss: 0.4162 train_acc: 0.8110 \n",
      "\u001B[1A\u001B[2K  93/387 39.18s - train_loss: 0.4162 train_acc: 0.8109 \n",
      "\u001B[1A\u001B[2K  94/387 39.56s - train_loss: 0.4159 train_acc: 0.8110 \n",
      "\u001B[1A\u001B[2K  95/387 39.94s - train_loss: 0.4158 train_acc: 0.8110 \n",
      "\u001B[1A\u001B[2K  96/387 40.32s - train_loss: 0.4152 train_acc: 0.8114 \n",
      "\u001B[1A\u001B[2K  97/387 40.70s - train_loss: 0.4153 train_acc: 0.8114 \n",
      "\u001B[1A\u001B[2K  98/387 41.08s - train_loss: 0.4151 train_acc: 0.8113 \n",
      "\u001B[1A\u001B[2K  99/387 41.47s - train_loss: 0.4145 train_acc: 0.8115 \n",
      "\u001B[1A\u001B[2K  100/387 41.84s - train_loss: 0.4141 train_acc: 0.8118 \n",
      "\u001B[1A\u001B[2K  101/387 42.22s - train_loss: 0.4137 train_acc: 0.8121 \n",
      "\u001B[1A\u001B[2K  102/387 42.59s - train_loss: 0.4132 train_acc: 0.8125 \n",
      "\u001B[1A\u001B[2K  103/387 42.96s - train_loss: 0.4128 train_acc: 0.8129 \n",
      "\u001B[1A\u001B[2K  104/387 43.35s - train_loss: 0.4121 train_acc: 0.8132 \n",
      "\u001B[1A\u001B[2K  105/387 43.74s - train_loss: 0.4118 train_acc: 0.8134 \n",
      "\u001B[1A\u001B[2K  106/387 44.11s - train_loss: 0.4114 train_acc: 0.8136 \n",
      "\u001B[1A\u001B[2K  107/387 44.49s - train_loss: 0.4108 train_acc: 0.8139 \n",
      "\u001B[1A\u001B[2K  108/387 44.86s - train_loss: 0.4100 train_acc: 0.8144 \n",
      "\u001B[1A\u001B[2K  109/387 45.24s - train_loss: 0.4096 train_acc: 0.8147 \n",
      "\u001B[1A\u001B[2K  110/387 45.63s - train_loss: 0.4090 train_acc: 0.8149 \n",
      "\u001B[1A\u001B[2K  111/387 46.02s - train_loss: 0.4081 train_acc: 0.8152 \n",
      "\u001B[1A\u001B[2K  112/387 46.40s - train_loss: 0.4076 train_acc: 0.8154 \n",
      "\u001B[1A\u001B[2K  113/387 46.78s - train_loss: 0.4074 train_acc: 0.8157 \n",
      "\u001B[1A\u001B[2K  114/387 47.16s - train_loss: 0.4068 train_acc: 0.8161 \n",
      "\u001B[1A\u001B[2K  115/387 47.55s - train_loss: 0.4060 train_acc: 0.8166 \n",
      "\u001B[1A\u001B[2K  116/387 47.92s - train_loss: 0.4054 train_acc: 0.8168 \n",
      "\u001B[1A\u001B[2K  117/387 48.31s - train_loss: 0.4047 train_acc: 0.8172 \n",
      "\u001B[1A\u001B[2K  118/387 48.70s - train_loss: 0.4040 train_acc: 0.8175 \n",
      "\u001B[1A\u001B[2K  119/387 49.08s - train_loss: 0.4032 train_acc: 0.8180 \n",
      "\u001B[1A\u001B[2K  120/387 49.45s - train_loss: 0.4031 train_acc: 0.8180 \n",
      "\u001B[1A\u001B[2K  121/387 49.83s - train_loss: 0.4021 train_acc: 0.8186 \n",
      "\u001B[1A\u001B[2K  122/387 50.20s - train_loss: 0.4014 train_acc: 0.8190 \n",
      "\u001B[1A\u001B[2K  123/387 50.58s - train_loss: 0.4008 train_acc: 0.8192 \n",
      "\u001B[1A\u001B[2K  124/387 50.97s - train_loss: 0.4006 train_acc: 0.8193 \n",
      "\u001B[1A\u001B[2K  125/387 51.35s - train_loss: 0.4002 train_acc: 0.8195 \n",
      "\u001B[1A\u001B[2K  126/387 51.73s - train_loss: 0.3995 train_acc: 0.8199 \n",
      "\u001B[1A\u001B[2K  127/387 52.10s - train_loss: 0.3990 train_acc: 0.8200 \n",
      "\u001B[1A\u001B[2K  128/387 52.48s - train_loss: 0.3985 train_acc: 0.8204 \n",
      "\u001B[1A\u001B[2K  129/387 52.86s - train_loss: 0.3982 train_acc: 0.8207 \n",
      "\u001B[1A\u001B[2K  130/387 53.25s - train_loss: 0.3974 train_acc: 0.8211 \n",
      "\u001B[1A\u001B[2K  131/387 53.64s - train_loss: 0.3966 train_acc: 0.8216 \n",
      "\u001B[1A\u001B[2K  132/387 54.02s - train_loss: 0.3960 train_acc: 0.8219 \n",
      "\u001B[1A\u001B[2K  133/387 54.40s - train_loss: 0.3956 train_acc: 0.8219 \n",
      "\u001B[1A\u001B[2K  134/387 54.78s - train_loss: 0.3947 train_acc: 0.8225 \n",
      "\u001B[1A\u001B[2K  135/387 55.16s - train_loss: 0.3941 train_acc: 0.8228 \n",
      "\u001B[1A\u001B[2K  136/387 55.53s - train_loss: 0.3937 train_acc: 0.8232 \n",
      "\u001B[1A\u001B[2K  137/387 55.91s - train_loss: 0.3935 train_acc: 0.8233 \n",
      "\u001B[1A\u001B[2K  138/387 56.29s - train_loss: 0.3930 train_acc: 0.8235 \n",
      "\u001B[1A\u001B[2K  139/387 56.68s - train_loss: 0.3926 train_acc: 0.8238 \n",
      "\u001B[1A\u001B[2K  140/387 57.06s - train_loss: 0.3921 train_acc: 0.8241 \n",
      "\u001B[1A\u001B[2K  141/387 57.43s - train_loss: 0.3920 train_acc: 0.8243 \n",
      "\u001B[1A\u001B[2K  142/387 57.81s - train_loss: 0.3917 train_acc: 0.8245 \n",
      "\u001B[1A\u001B[2K  143/387 58.19s - train_loss: 0.3910 train_acc: 0.8249 \n",
      "\u001B[1A\u001B[2K  144/387 58.56s - train_loss: 0.3904 train_acc: 0.8252 \n",
      "\u001B[1A\u001B[2K  145/387 58.93s - train_loss: 0.3903 train_acc: 0.8253 \n",
      "\u001B[1A\u001B[2K  146/387 59.31s - train_loss: 0.3900 train_acc: 0.8254 \n",
      "\u001B[1A\u001B[2K  147/387 59.69s - train_loss: 0.3898 train_acc: 0.8257 \n",
      "\u001B[1A\u001B[2K  148/387 60.07s - train_loss: 0.3892 train_acc: 0.8260 \n",
      "\u001B[1A\u001B[2K  149/387 60.45s - train_loss: 0.3893 train_acc: 0.8261 \n",
      "\u001B[1A\u001B[2K  150/387 60.83s - train_loss: 0.3887 train_acc: 0.8264 \n",
      "\u001B[1A\u001B[2K  151/387 61.22s - train_loss: 0.3883 train_acc: 0.8265 \n",
      "\u001B[1A\u001B[2K  152/387 61.61s - train_loss: 0.3878 train_acc: 0.8267 \n",
      "\u001B[1A\u001B[2K  153/387 61.99s - train_loss: 0.3875 train_acc: 0.8270 \n",
      "\u001B[1A\u001B[2K  154/387 62.37s - train_loss: 0.3868 train_acc: 0.8273 \n",
      "\u001B[1A\u001B[2K  155/387 62.75s - train_loss: 0.3863 train_acc: 0.8276 \n",
      "\u001B[1A\u001B[2K  156/387 63.12s - train_loss: 0.3858 train_acc: 0.8279 \n",
      "\u001B[1A\u001B[2K  157/387 63.49s - train_loss: 0.3854 train_acc: 0.8283 \n",
      "\u001B[1A\u001B[2K  158/387 63.87s - train_loss: 0.3848 train_acc: 0.8286 \n",
      "\u001B[1A\u001B[2K  159/387 64.24s - train_loss: 0.3847 train_acc: 0.8287 \n",
      "\u001B[1A\u001B[2K  160/387 64.62s - train_loss: 0.3844 train_acc: 0.8289 \n",
      "\u001B[1A\u001B[2K  161/387 65.00s - train_loss: 0.3838 train_acc: 0.8291 \n",
      "\u001B[1A\u001B[2K  162/387 65.37s - train_loss: 0.3835 train_acc: 0.8293 \n",
      "\u001B[1A\u001B[2K  163/387 65.75s - train_loss: 0.3831 train_acc: 0.8295 \n",
      "\u001B[1A\u001B[2K  164/387 66.14s - train_loss: 0.3829 train_acc: 0.8297 \n",
      "\u001B[1A\u001B[2K  165/387 66.52s - train_loss: 0.3826 train_acc: 0.8298 \n",
      "\u001B[1A\u001B[2K  166/387 66.90s - train_loss: 0.3822 train_acc: 0.8300 \n",
      "\u001B[1A\u001B[2K  167/387 67.29s - train_loss: 0.3824 train_acc: 0.8299 \n",
      "\u001B[1A\u001B[2K  168/387 67.67s - train_loss: 0.3822 train_acc: 0.8300 \n",
      "\u001B[1A\u001B[2K  169/387 68.05s - train_loss: 0.3825 train_acc: 0.8298 \n",
      "\u001B[1A\u001B[2K  170/387 68.44s - train_loss: 0.3825 train_acc: 0.8298 \n",
      "\u001B[1A\u001B[2K  171/387 68.82s - train_loss: 0.3821 train_acc: 0.8301 \n",
      "\u001B[1A\u001B[2K  172/387 69.20s - train_loss: 0.3817 train_acc: 0.8303 \n",
      "\u001B[1A\u001B[2K  173/387 69.59s - train_loss: 0.3814 train_acc: 0.8305 \n",
      "\u001B[1A\u001B[2K  174/387 69.97s - train_loss: 0.3814 train_acc: 0.8305 \n",
      "\u001B[1A\u001B[2K  175/387 70.35s - train_loss: 0.3815 train_acc: 0.8305 \n",
      "\u001B[1A\u001B[2K  176/387 70.73s - train_loss: 0.3813 train_acc: 0.8306 \n",
      "\u001B[1A\u001B[2K  177/387 71.12s - train_loss: 0.3809 train_acc: 0.8309 \n",
      "\u001B[1A\u001B[2K  178/387 71.52s - train_loss: 0.3808 train_acc: 0.8309 \n",
      "\u001B[1A\u001B[2K  179/387 71.91s - train_loss: 0.3807 train_acc: 0.8310 \n",
      "\u001B[1A\u001B[2K  180/387 72.30s - train_loss: 0.3804 train_acc: 0.8311 \n",
      "\u001B[1A\u001B[2K  181/387 72.69s - train_loss: 0.3801 train_acc: 0.8313 \n",
      "\u001B[1A\u001B[2K  182/387 73.08s - train_loss: 0.3798 train_acc: 0.8315 \n",
      "\u001B[1A\u001B[2K  183/387 73.48s - train_loss: 0.3798 train_acc: 0.8315 \n",
      "\u001B[1A\u001B[2K  184/387 73.87s - train_loss: 0.3797 train_acc: 0.8316 \n",
      "\u001B[1A\u001B[2K  185/387 74.28s - train_loss: 0.3794 train_acc: 0.8317 \n",
      "\u001B[1A\u001B[2K  186/387 74.68s - train_loss: 0.3789 train_acc: 0.8320 \n",
      "\u001B[1A\u001B[2K  187/387 75.09s - train_loss: 0.3790 train_acc: 0.8319 \n",
      "\u001B[1A\u001B[2K  188/387 75.50s - train_loss: 0.3786 train_acc: 0.8321 \n",
      "\u001B[1A\u001B[2K  189/387 75.91s - train_loss: 0.3784 train_acc: 0.8322 \n",
      "\u001B[1A\u001B[2K  190/387 76.33s - train_loss: 0.3781 train_acc: 0.8324 \n",
      "\u001B[1A\u001B[2K  191/387 76.75s - train_loss: 0.3780 train_acc: 0.8326 \n",
      "\u001B[1A\u001B[2K  192/387 77.17s - train_loss: 0.3775 train_acc: 0.8328 \n",
      "\u001B[1A\u001B[2K  193/387 77.59s - train_loss: 0.3771 train_acc: 0.8331 \n",
      "\u001B[1A\u001B[2K  194/387 78.02s - train_loss: 0.3767 train_acc: 0.8331 \n",
      "\u001B[1A\u001B[2K  195/387 78.45s - train_loss: 0.3763 train_acc: 0.8334 \n",
      "\u001B[1A\u001B[2K  196/387 78.89s - train_loss: 0.3760 train_acc: 0.8335 \n",
      "\u001B[1A\u001B[2K  197/387 79.33s - train_loss: 0.3759 train_acc: 0.8336 \n",
      "\u001B[1A\u001B[2K  198/387 79.79s - train_loss: 0.3757 train_acc: 0.8337 \n",
      "\u001B[1A\u001B[2K  199/387 80.24s - train_loss: 0.3755 train_acc: 0.8338 \n",
      "\u001B[1A\u001B[2K  200/387 80.70s - train_loss: 0.3754 train_acc: 0.8337 \n",
      "\u001B[1A\u001B[2K  201/387 81.17s - train_loss: 0.3751 train_acc: 0.8340 \n",
      "\u001B[1A\u001B[2K  202/387 81.65s - train_loss: 0.3749 train_acc: 0.8341 \n",
      "\u001B[1A\u001B[2K  203/387 82.13s - train_loss: 0.3748 train_acc: 0.8341 \n",
      "\u001B[1A\u001B[2K  204/387 82.63s - train_loss: 0.3745 train_acc: 0.8343 \n",
      "\u001B[1A\u001B[2K  205/387 83.12s - train_loss: 0.3741 train_acc: 0.8346 \n",
      "\u001B[1A\u001B[2K  206/387 83.62s - train_loss: 0.3736 train_acc: 0.8350 \n",
      "\u001B[1A\u001B[2K  207/387 84.11s - train_loss: 0.3735 train_acc: 0.8351 \n",
      "\u001B[1A\u001B[2K  208/387 84.61s - train_loss: 0.3732 train_acc: 0.8353 \n",
      "\u001B[1A\u001B[2K  209/387 85.11s - train_loss: 0.3730 train_acc: 0.8353 \n",
      "\u001B[1A\u001B[2K  210/387 85.60s - train_loss: 0.3729 train_acc: 0.8354 \n",
      "\u001B[1A\u001B[2K  211/387 86.11s - train_loss: 0.3727 train_acc: 0.8356 \n",
      "\u001B[1A\u001B[2K  212/387 86.61s - train_loss: 0.3723 train_acc: 0.8357 \n",
      "\u001B[1A\u001B[2K  213/387 87.12s - train_loss: 0.3721 train_acc: 0.8359 \n",
      "\u001B[1A\u001B[2K  214/387 87.63s - train_loss: 0.3721 train_acc: 0.8360 \n",
      "\u001B[1A\u001B[2K  215/387 88.16s - train_loss: 0.3718 train_acc: 0.8360 \n",
      "\u001B[1A\u001B[2K  216/387 88.70s - train_loss: 0.3716 train_acc: 0.8361 \n",
      "\u001B[1A\u001B[2K  217/387 89.24s - train_loss: 0.3713 train_acc: 0.8363 \n",
      "\u001B[1A\u001B[2K  218/387 89.78s - train_loss: 0.3710 train_acc: 0.8364 \n",
      "\u001B[1A\u001B[2K  219/387 90.32s - train_loss: 0.3707 train_acc: 0.8366 \n",
      "\u001B[1A\u001B[2K  220/387 90.87s - train_loss: 0.3704 train_acc: 0.8367 \n",
      "\u001B[1A\u001B[2K  221/387 91.43s - train_loss: 0.3701 train_acc: 0.8369 \n",
      "\u001B[1A\u001B[2K  222/387 91.99s - train_loss: 0.3698 train_acc: 0.8370 \n",
      "\u001B[1A\u001B[2K  223/387 92.55s - train_loss: 0.3692 train_acc: 0.8373 \n",
      "\u001B[1A\u001B[2K  224/387 93.11s - train_loss: 0.3689 train_acc: 0.8375 \n",
      "\u001B[1A\u001B[2K  225/387 93.68s - train_loss: 0.3688 train_acc: 0.8375 \n",
      "\u001B[1A\u001B[2K  226/387 94.26s - train_loss: 0.3686 train_acc: 0.8376 \n",
      "\u001B[1A\u001B[2K  227/387 94.86s - train_loss: 0.3684 train_acc: 0.8377 \n",
      "\u001B[1A\u001B[2K  228/387 95.47s - train_loss: 0.3681 train_acc: 0.8379 \n",
      "\u001B[1A\u001B[2K  229/387 96.07s - train_loss: 0.3677 train_acc: 0.8382 \n",
      "\u001B[1A\u001B[2K  230/387 96.67s - train_loss: 0.3674 train_acc: 0.8384 \n",
      "\u001B[1A\u001B[2K  231/387 97.27s - train_loss: 0.3675 train_acc: 0.8383 \n",
      "\u001B[1A\u001B[2K  232/387 97.88s - train_loss: 0.3672 train_acc: 0.8384 \n",
      "\u001B[1A\u001B[2K  233/387 98.48s - train_loss: 0.3671 train_acc: 0.8386 \n",
      "\u001B[1A\u001B[2K  234/387 99.09s - train_loss: 0.3669 train_acc: 0.8387 \n",
      "\u001B[1A\u001B[2K  235/387 99.70s - train_loss: 0.3667 train_acc: 0.8389 \n",
      "\u001B[1A\u001B[2K  236/387 100.34s - train_loss: 0.3664 train_acc: 0.8391 \n",
      "\u001B[1A\u001B[2K  237/387 101.00s - train_loss: 0.3661 train_acc: 0.8392 \n",
      "\u001B[1A\u001B[2K  238/387 101.68s - train_loss: 0.3658 train_acc: 0.8394 \n",
      "\u001B[1A\u001B[2K  239/387 102.36s - train_loss: 0.3655 train_acc: 0.8397 \n",
      "\u001B[1A\u001B[2K  240/387 103.03s - train_loss: 0.3651 train_acc: 0.8398 \n",
      "\u001B[1A\u001B[2K  241/387 103.70s - train_loss: 0.3648 train_acc: 0.8400 \n",
      "\u001B[1A\u001B[2K  242/387 104.36s - train_loss: 0.3645 train_acc: 0.8402 \n",
      "\u001B[1A\u001B[2K  243/387 105.02s - train_loss: 0.3644 train_acc: 0.8403 \n",
      "\u001B[1A\u001B[2K  244/387 105.69s - train_loss: 0.3641 train_acc: 0.8405 \n",
      "\u001B[1A\u001B[2K  245/387 106.38s - train_loss: 0.3637 train_acc: 0.8406 \n",
      "\u001B[1A\u001B[2K  246/387 107.10s - train_loss: 0.3635 train_acc: 0.8408 \n",
      "\u001B[1A\u001B[2K  247/387 107.86s - train_loss: 0.3633 train_acc: 0.8409 \n",
      "\u001B[1A\u001B[2K  248/387 108.63s - train_loss: 0.3631 train_acc: 0.8411 \n",
      "\u001B[1A\u001B[2K  249/387 109.37s - train_loss: 0.3629 train_acc: 0.8411 \n",
      "\u001B[1A\u001B[2K  250/387 110.09s - train_loss: 0.3628 train_acc: 0.8412 \n",
      "\u001B[1A\u001B[2K  251/387 110.78s - train_loss: 0.3624 train_acc: 0.8414 \n",
      "\u001B[1A\u001B[2K  252/387 111.46s - train_loss: 0.3622 train_acc: 0.8416 \n",
      "\u001B[1A\u001B[2K  253/387 112.14s - train_loss: 0.3621 train_acc: 0.8416 \n",
      "\u001B[1A\u001B[2K  254/387 112.83s - train_loss: 0.3618 train_acc: 0.8417 \n",
      "\u001B[1A\u001B[2K  255/387 113.54s - train_loss: 0.3615 train_acc: 0.8419 \n",
      "\u001B[1A\u001B[2K  256/387 114.29s - train_loss: 0.3613 train_acc: 0.8420 \n",
      "\u001B[1A\u001B[2K  257/387 115.07s - train_loss: 0.3609 train_acc: 0.8422 \n",
      "\u001B[1A\u001B[2K  258/387 115.85s - train_loss: 0.3608 train_acc: 0.8423 \n",
      "\u001B[1A\u001B[2K  259/387 116.60s - train_loss: 0.3606 train_acc: 0.8424 \n",
      "\u001B[1A\u001B[2K  260/387 117.34s - train_loss: 0.3604 train_acc: 0.8424 \n",
      "\u001B[1A\u001B[2K  261/387 118.06s - train_loss: 0.3600 train_acc: 0.8425 \n",
      "\u001B[1A\u001B[2K  262/387 118.78s - train_loss: 0.3598 train_acc: 0.8427 \n",
      "\u001B[1A\u001B[2K  263/387 119.50s - train_loss: 0.3597 train_acc: 0.8427 \n",
      "\u001B[1A\u001B[2K  264/387 120.26s - train_loss: 0.3595 train_acc: 0.8429 \n",
      "\u001B[1A\u001B[2K  265/387 121.07s - train_loss: 0.3592 train_acc: 0.8430 \n",
      "\u001B[1A\u001B[2K  266/387 121.94s - train_loss: 0.3590 train_acc: 0.8432 \n",
      "\u001B[1A\u001B[2K  267/387 122.78s - train_loss: 0.3587 train_acc: 0.8433 \n",
      "\u001B[1A\u001B[2K  268/387 123.59s - train_loss: 0.3585 train_acc: 0.8434 \n",
      "\u001B[1A\u001B[2K  269/387 124.34s - train_loss: 0.3583 train_acc: 0.8436 \n",
      "\u001B[1A\u001B[2K  270/387 125.06s - train_loss: 0.3579 train_acc: 0.8438 \n",
      "\u001B[1A\u001B[2K  271/387 125.78s - train_loss: 0.3576 train_acc: 0.8440 \n",
      "\u001B[1A\u001B[2K  272/387 126.50s - train_loss: 0.3571 train_acc: 0.8442 \n",
      "\u001B[1A\u001B[2K  273/387 127.25s - train_loss: 0.3569 train_acc: 0.8444 \n",
      "\u001B[1A\u001B[2K  274/387 128.05s - train_loss: 0.3568 train_acc: 0.8445 \n",
      "\u001B[1A\u001B[2K  275/387 128.84s - train_loss: 0.3565 train_acc: 0.8447 \n",
      "\u001B[1A\u001B[2K  276/387 129.61s - train_loss: 0.3562 train_acc: 0.8448 \n",
      "\u001B[1A\u001B[2K  277/387 130.36s - train_loss: 0.3560 train_acc: 0.8449 \n",
      "\u001B[1A\u001B[2K  278/387 131.10s - train_loss: 0.3556 train_acc: 0.8452 \n",
      "\u001B[1A\u001B[2K  279/387 131.84s - train_loss: 0.3555 train_acc: 0.8452 \n",
      "\u001B[1A\u001B[2K  280/387 132.58s - train_loss: 0.3551 train_acc: 0.8454 \n",
      "\u001B[1A\u001B[2K  281/387 133.34s - train_loss: 0.3550 train_acc: 0.8455 \n",
      "\u001B[1A\u001B[2K  282/387 134.14s - train_loss: 0.3547 train_acc: 0.8457 \n",
      "\u001B[1A\u001B[2K  283/387 134.96s - train_loss: 0.3543 train_acc: 0.8459 \n",
      "\u001B[1A\u001B[2K  284/387 135.76s - train_loss: 0.3540 train_acc: 0.8460 \n",
      "\u001B[1A\u001B[2K  285/387 136.55s - train_loss: 0.3538 train_acc: 0.8461 \n",
      "\u001B[1A\u001B[2K  286/387 137.34s - train_loss: 0.3535 train_acc: 0.8463 \n",
      "\u001B[1A\u001B[2K  287/387 138.08s - train_loss: 0.3531 train_acc: 0.8465 \n",
      "\u001B[1A\u001B[2K  288/387 138.80s - train_loss: 0.3528 train_acc: 0.8466 \n",
      "\u001B[1A\u001B[2K  289/387 139.53s - train_loss: 0.3527 train_acc: 0.8467 \n",
      "\u001B[1A\u001B[2K  290/387 140.27s - train_loss: 0.3526 train_acc: 0.8468 \n",
      "\u001B[1A\u001B[2K  291/387 141.04s - train_loss: 0.3523 train_acc: 0.8470 \n",
      "\u001B[1A\u001B[2K  292/387 141.82s - train_loss: 0.3520 train_acc: 0.8471 \n",
      "\u001B[1A\u001B[2K  293/387 142.56s - train_loss: 0.3518 train_acc: 0.8472 \n",
      "\u001B[1A\u001B[2K  294/387 143.29s - train_loss: 0.3516 train_acc: 0.8473 \n",
      "\u001B[1A\u001B[2K  295/387 144.01s - train_loss: 0.3513 train_acc: 0.8475 \n",
      "\u001B[1A\u001B[2K  296/387 144.72s - train_loss: 0.3511 train_acc: 0.8476 \n",
      "\u001B[1A\u001B[2K  297/387 145.43s - train_loss: 0.3508 train_acc: 0.8478 \n",
      "\u001B[1A\u001B[2K  298/387 146.14s - train_loss: 0.3506 train_acc: 0.8479 \n",
      "\u001B[1A\u001B[2K  299/387 146.90s - train_loss: 0.3503 train_acc: 0.8480 \n",
      "\u001B[1A\u001B[2K  300/387 147.67s - train_loss: 0.3501 train_acc: 0.8481 \n",
      "\u001B[1A\u001B[2K  301/387 148.45s - train_loss: 0.3500 train_acc: 0.8481 \n",
      "\u001B[1A\u001B[2K  302/387 149.19s - train_loss: 0.3497 train_acc: 0.8483 \n",
      "\u001B[1A\u001B[2K  303/387 149.91s - train_loss: 0.3496 train_acc: 0.8484 \n",
      "\u001B[1A\u001B[2K  304/387 150.62s - train_loss: 0.3493 train_acc: 0.8485 \n",
      "\u001B[1A\u001B[2K  305/387 151.33s - train_loss: 0.3491 train_acc: 0.8486 \n",
      "\u001B[1A\u001B[2K  306/387 152.05s - train_loss: 0.3488 train_acc: 0.8488 \n",
      "\u001B[1A\u001B[2K  307/387 152.77s - train_loss: 0.3485 train_acc: 0.8490 \n",
      "\u001B[1A\u001B[2K  308/387 153.50s - train_loss: 0.3483 train_acc: 0.8491 \n",
      "\u001B[1A\u001B[2K  309/387 154.24s - train_loss: 0.3479 train_acc: 0.8492 \n",
      "\u001B[1A\u001B[2K  310/387 154.97s - train_loss: 0.3477 train_acc: 0.8493 \n",
      "\u001B[1A\u001B[2K  311/387 155.68s - train_loss: 0.3475 train_acc: 0.8494 \n",
      "\u001B[1A\u001B[2K  312/387 156.37s - train_loss: 0.3473 train_acc: 0.8496 \n",
      "\u001B[1A\u001B[2K  313/387 157.05s - train_loss: 0.3470 train_acc: 0.8497 \n",
      "\u001B[1A\u001B[2K  314/387 157.73s - train_loss: 0.3469 train_acc: 0.8498 \n",
      "\u001B[1A\u001B[2K  315/387 158.41s - train_loss: 0.3465 train_acc: 0.8500 \n",
      "\u001B[1A\u001B[2K  316/387 159.08s - train_loss: 0.3463 train_acc: 0.8501 \n",
      "\u001B[1A\u001B[2K  317/387 159.79s - train_loss: 0.3460 train_acc: 0.8503 \n",
      "\u001B[1A\u001B[2K  318/387 160.52s - train_loss: 0.3457 train_acc: 0.8505 \n",
      "\u001B[1A\u001B[2K  319/387 161.27s - train_loss: 0.3455 train_acc: 0.8506 \n",
      "\u001B[1A\u001B[2K  320/387 162.00s - train_loss: 0.3453 train_acc: 0.8507 \n",
      "\u001B[1A\u001B[2K  321/387 162.71s - train_loss: 0.3452 train_acc: 0.8508 \n",
      "\u001B[1A\u001B[2K  322/387 163.40s - train_loss: 0.3450 train_acc: 0.8509 \n",
      "\u001B[1A\u001B[2K  323/387 164.08s - train_loss: 0.3448 train_acc: 0.8509 \n",
      "\u001B[1A\u001B[2K  324/387 164.76s - train_loss: 0.3446 train_acc: 0.8511 \n",
      "\u001B[1A\u001B[2K  325/387 165.45s - train_loss: 0.3444 train_acc: 0.8512 \n",
      "\u001B[1A\u001B[2K  326/387 166.17s - train_loss: 0.3441 train_acc: 0.8514 \n",
      "\u001B[1A\u001B[2K  327/387 166.90s - train_loss: 0.3439 train_acc: 0.8515 \n",
      "\u001B[1A\u001B[2K  328/387 167.65s - train_loss: 0.3436 train_acc: 0.8516 \n",
      "\u001B[1A\u001B[2K  329/387 168.38s - train_loss: 0.3434 train_acc: 0.8518 \n",
      "\u001B[1A\u001B[2K  330/387 169.08s - train_loss: 0.3432 train_acc: 0.8519 \n",
      "\u001B[1A\u001B[2K  331/387 169.77s - train_loss: 0.3430 train_acc: 0.8520 \n",
      "\u001B[1A\u001B[2K  332/387 170.44s - train_loss: 0.3429 train_acc: 0.8520 \n",
      "\u001B[1A\u001B[2K  333/387 171.10s - train_loss: 0.3427 train_acc: 0.8521 \n",
      "\u001B[1A\u001B[2K  334/387 171.76s - train_loss: 0.3424 train_acc: 0.8523 \n",
      "\u001B[1A\u001B[2K  335/387 172.43s - train_loss: 0.3421 train_acc: 0.8524 \n",
      "\u001B[1A\u001B[2K  336/387 173.12s - train_loss: 0.3417 train_acc: 0.8526 \n",
      "\u001B[1A\u001B[2K  337/387 173.84s - train_loss: 0.3416 train_acc: 0.8527 \n",
      "\u001B[1A\u001B[2K  338/387 174.57s - train_loss: 0.3415 train_acc: 0.8527 \n",
      "\u001B[1A\u001B[2K  339/387 175.29s - train_loss: 0.3412 train_acc: 0.8528 \n",
      "\u001B[1A\u001B[2K  340/387 175.98s - train_loss: 0.3410 train_acc: 0.8529 \n",
      "\u001B[1A\u001B[2K  341/387 176.66s - train_loss: 0.3408 train_acc: 0.8531 \n",
      "\u001B[1A\u001B[2K  342/387 177.34s - train_loss: 0.3405 train_acc: 0.8532 \n",
      "\u001B[1A\u001B[2K  343/387 178.01s - train_loss: 0.3402 train_acc: 0.8533 \n",
      "\u001B[1A\u001B[2K  344/387 178.67s - train_loss: 0.3401 train_acc: 0.8534 \n",
      "\u001B[1A\u001B[2K  345/387 179.36s - train_loss: 0.3399 train_acc: 0.8535 \n",
      "\u001B[1A\u001B[2K  346/387 180.08s - train_loss: 0.3396 train_acc: 0.8536 \n",
      "\u001B[1A\u001B[2K  347/387 180.83s - train_loss: 0.3394 train_acc: 0.8537 \n",
      "\u001B[1A\u001B[2K  348/387 181.58s - train_loss: 0.3392 train_acc: 0.8538 \n",
      "\u001B[1A\u001B[2K  349/387 182.31s - train_loss: 0.3390 train_acc: 0.8539 \n",
      "\u001B[1A\u001B[2K  350/387 182.99s - train_loss: 0.3387 train_acc: 0.8541 \n",
      "\u001B[1A\u001B[2K  351/387 183.66s - train_loss: 0.3385 train_acc: 0.8541 \n",
      "\u001B[1A\u001B[2K  352/387 184.32s - train_loss: 0.3384 train_acc: 0.8542 \n",
      "\u001B[1A\u001B[2K  353/387 184.97s - train_loss: 0.3382 train_acc: 0.8543 \n",
      "\u001B[1A\u001B[2K  354/387 185.61s - train_loss: 0.3380 train_acc: 0.8544 \n",
      "\u001B[1A\u001B[2K  355/387 186.29s - train_loss: 0.3377 train_acc: 0.8546 \n",
      "\u001B[1A\u001B[2K  356/387 186.98s - train_loss: 0.3376 train_acc: 0.8547 \n",
      "\u001B[1A\u001B[2K  357/387 187.68s - train_loss: 0.3373 train_acc: 0.8548 \n",
      "\u001B[1A\u001B[2K  358/387 188.37s - train_loss: 0.3371 train_acc: 0.8549 \n",
      "\u001B[1A\u001B[2K  359/387 189.05s - train_loss: 0.3369 train_acc: 0.8550 \n",
      "\u001B[1A\u001B[2K  360/387 189.72s - train_loss: 0.3367 train_acc: 0.8552 \n",
      "\u001B[1A\u001B[2K  361/387 190.39s - train_loss: 0.3365 train_acc: 0.8553 \n",
      "\u001B[1A\u001B[2K  362/387 191.05s - train_loss: 0.3363 train_acc: 0.8554 \n",
      "\u001B[1A\u001B[2K  363/387 191.71s - train_loss: 0.3362 train_acc: 0.8555 \n",
      "\u001B[1A\u001B[2K  364/387 192.37s - train_loss: 0.3362 train_acc: 0.8555 \n",
      "\u001B[1A\u001B[2K  365/387 193.07s - train_loss: 0.3360 train_acc: 0.8555 \n",
      "\u001B[1A\u001B[2K  366/387 193.78s - train_loss: 0.3358 train_acc: 0.8556 \n",
      "\u001B[1A\u001B[2K  367/387 194.49s - train_loss: 0.3357 train_acc: 0.8557 \n",
      "\u001B[1A\u001B[2K  368/387 195.20s - train_loss: 0.3357 train_acc: 0.8557 \n",
      "\u001B[1A\u001B[2K  369/387 195.89s - train_loss: 0.3354 train_acc: 0.8559 \n",
      "\u001B[1A\u001B[2K  370/387 196.58s - train_loss: 0.3353 train_acc: 0.8560 \n",
      "\u001B[1A\u001B[2K  371/387 197.26s - train_loss: 0.3352 train_acc: 0.8560 \n",
      "\u001B[1A\u001B[2K  372/387 197.91s - train_loss: 0.3351 train_acc: 0.8560 \n",
      "\u001B[1A\u001B[2K  373/387 198.57s - train_loss: 0.3353 train_acc: 0.8560 \n",
      "\u001B[1A\u001B[2K  374/387 199.24s - train_loss: 0.3351 train_acc: 0.8561 \n",
      "\u001B[1A\u001B[2K  375/387 199.93s - train_loss: 0.3349 train_acc: 0.8562 \n",
      "\u001B[1A\u001B[2K  376/387 200.63s - train_loss: 0.3347 train_acc: 0.8563 \n",
      "\u001B[1A\u001B[2K  377/387 201.31s - train_loss: 0.3346 train_acc: 0.8563 \n",
      "\u001B[1A\u001B[2K  378/387 201.98s - train_loss: 0.3344 train_acc: 0.8565 \n",
      "\u001B[1A\u001B[2K  379/387 202.64s - train_loss: 0.3343 train_acc: 0.8566 \n",
      "\u001B[1A\u001B[2K  380/387 203.29s - train_loss: 0.3341 train_acc: 0.8566 \n",
      "\u001B[1A\u001B[2K  381/387 203.92s - train_loss: 0.3339 train_acc: 0.8568 \n",
      "\u001B[1A\u001B[2K  382/387 204.52s - train_loss: 0.3337 train_acc: 0.8569 \n",
      "\u001B[1A\u001B[2K  383/387 205.12s - train_loss: 0.3335 train_acc: 0.8569 \n",
      "\u001B[1A\u001B[2K  384/387 205.73s - train_loss: 0.3335 train_acc: 0.8569 \n",
      "\u001B[1A\u001B[2K  385/387 206.36s - train_loss: 0.3333 train_acc: 0.8571 \n",
      "\u001B[1A\u001B[2K  386/387 207.00s - train_loss: 0.3331 train_acc: 0.8572 \n",
      "\u001B[1A\u001B[2K  387/387 207.48s - train_loss: 0.3329 train_acc: 0.8572 \n",
      "\u001B[1A\u001B[2K  255.24s - train_loss: 0.3329 train_acc: 0.8572 std_val_loss: 0.0005 std_val_acc: 0.8991 \n",
      "2/3 epoch:\n",
      "\n",
      "\u001B[1A\u001B[2K  1/387 3.01s - train_loss: 0.2450 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  2/387 3.38s - train_loss: 0.2704 train_acc: 0.8916 \n",
      "\u001B[1A\u001B[2K  3/387 3.75s - train_loss: 0.2704 train_acc: 0.8913 \n",
      "\u001B[1A\u001B[2K  4/387 4.12s - train_loss: 0.2549 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  5/387 4.49s - train_loss: 0.2611 train_acc: 0.8961 \n",
      "\u001B[1A\u001B[2K  6/387 4.87s - train_loss: 0.2624 train_acc: 0.8945 \n",
      "\u001B[1A\u001B[2K  7/387 5.25s - train_loss: 0.2618 train_acc: 0.8943 \n",
      "\u001B[1A\u001B[2K  8/387 5.62s - train_loss: 0.2639 train_acc: 0.8931 \n",
      "\u001B[1A\u001B[2K  9/387 5.99s - train_loss: 0.2662 train_acc: 0.8915 \n",
      "\u001B[1A\u001B[2K  10/387 6.36s - train_loss: 0.2669 train_acc: 0.8914 \n",
      "\u001B[1A\u001B[2K  11/387 6.73s - train_loss: 0.2633 train_acc: 0.8933 \n",
      "\u001B[1A\u001B[2K  12/387 7.10s - train_loss: 0.2625 train_acc: 0.8934 \n",
      "\u001B[1A\u001B[2K  13/387 7.47s - train_loss: 0.2616 train_acc: 0.8939 \n",
      "\u001B[1A\u001B[2K  14/387 7.84s - train_loss: 0.2611 train_acc: 0.8948 \n",
      "\u001B[1A\u001B[2K  15/387 8.21s - train_loss: 0.2575 train_acc: 0.8962 \n",
      "\u001B[1A\u001B[2K  16/387 8.58s - train_loss: 0.2563 train_acc: 0.8970 \n",
      "\u001B[1A\u001B[2K  17/387 8.95s - train_loss: 0.2557 train_acc: 0.8965 \n",
      "\u001B[1A\u001B[2K  18/387 9.32s - train_loss: 0.2548 train_acc: 0.8967 \n",
      "\u001B[1A\u001B[2K  19/387 9.69s - train_loss: 0.2569 train_acc: 0.8960 \n",
      "\u001B[1A\u001B[2K  20/387 10.07s - train_loss: 0.2560 train_acc: 0.8965 \n",
      "\u001B[1A\u001B[2K  21/387 10.45s - train_loss: 0.2530 train_acc: 0.8975 \n",
      "\u001B[1A\u001B[2K  22/387 10.83s - train_loss: 0.2522 train_acc: 0.8980 \n",
      "\u001B[1A\u001B[2K  23/387 11.23s - train_loss: 0.2522 train_acc: 0.8978 \n",
      "\u001B[1A\u001B[2K  24/387 11.62s - train_loss: 0.2514 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  25/387 12.02s - train_loss: 0.2503 train_acc: 0.8987 \n",
      "\u001B[1A\u001B[2K  26/387 12.42s - train_loss: 0.2491 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  27/387 12.82s - train_loss: 0.2471 train_acc: 0.9000 \n",
      "\u001B[1A\u001B[2K  28/387 13.23s - train_loss: 0.2493 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  29/387 13.63s - train_loss: 0.2484 train_acc: 0.8991 \n",
      "\u001B[1A\u001B[2K  30/387 14.05s - train_loss: 0.2480 train_acc: 0.8993 \n",
      "\u001B[1A\u001B[2K  31/387 14.46s - train_loss: 0.2479 train_acc: 0.8994 \n",
      "\u001B[1A\u001B[2K  32/387 14.87s - train_loss: 0.2502 train_acc: 0.8987 \n",
      "\u001B[1A\u001B[2K  33/387 15.29s - train_loss: 0.2498 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  34/387 15.71s - train_loss: 0.2483 train_acc: 0.8994 \n",
      "\u001B[1A\u001B[2K  35/387 16.13s - train_loss: 0.2478 train_acc: 0.8997 \n",
      "\u001B[1A\u001B[2K  36/387 16.55s - train_loss: 0.2476 train_acc: 0.8997 \n",
      "\u001B[1A\u001B[2K  37/387 16.97s - train_loss: 0.2484 train_acc: 0.8992 \n",
      "\u001B[1A\u001B[2K  38/387 17.40s - train_loss: 0.2486 train_acc: 0.8993 \n",
      "\u001B[1A\u001B[2K  39/387 17.82s - train_loss: 0.2484 train_acc: 0.8992 \n",
      "\u001B[1A\u001B[2K  40/387 18.25s - train_loss: 0.2487 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  41/387 18.69s - train_loss: 0.2491 train_acc: 0.8987 \n",
      "\u001B[1A\u001B[2K  42/387 19.12s - train_loss: 0.2485 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  43/387 19.55s - train_loss: 0.2480 train_acc: 0.8992 \n",
      "\u001B[1A\u001B[2K  44/387 19.98s - train_loss: 0.2485 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  45/387 20.41s - train_loss: 0.2492 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  46/387 20.84s - train_loss: 0.2496 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  47/387 21.27s - train_loss: 0.2494 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  48/387 21.71s - train_loss: 0.2492 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  49/387 22.14s - train_loss: 0.2495 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  50/387 22.58s - train_loss: 0.2488 train_acc: 0.8989 \n",
      "\u001B[1A\u001B[2K  51/387 23.02s - train_loss: 0.2481 train_acc: 0.8992 \n",
      "\u001B[1A\u001B[2K  52/387 23.47s - train_loss: 0.2496 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  53/387 23.92s - train_loss: 0.2500 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  54/387 24.38s - train_loss: 0.2507 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  55/387 24.83s - train_loss: 0.2512 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  56/387 25.29s - train_loss: 0.2517 train_acc: 0.8981 \n",
      "\u001B[1A\u001B[2K  57/387 25.74s - train_loss: 0.2519 train_acc: 0.8981 \n",
      "\u001B[1A\u001B[2K  58/387 26.20s - train_loss: 0.2513 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  59/387 26.66s - train_loss: 0.2515 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  60/387 27.12s - train_loss: 0.2518 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  61/387 27.58s - train_loss: 0.2514 train_acc: 0.8982 \n",
      "\u001B[1A\u001B[2K  62/387 28.03s - train_loss: 0.2512 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  63/387 28.50s - train_loss: 0.2511 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  64/387 28.96s - train_loss: 0.2511 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  65/387 29.43s - train_loss: 0.2508 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  66/387 29.90s - train_loss: 0.2509 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  67/387 30.39s - train_loss: 0.2519 train_acc: 0.8980 \n",
      "\u001B[1A\u001B[2K  68/387 30.87s - train_loss: 0.2521 train_acc: 0.8980 \n",
      "\u001B[1A\u001B[2K  69/387 31.37s - train_loss: 0.2519 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  70/387 31.86s - train_loss: 0.2518 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  71/387 32.36s - train_loss: 0.2517 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  72/387 32.85s - train_loss: 0.2521 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  73/387 33.35s - train_loss: 0.2518 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  74/387 33.84s - train_loss: 0.2517 train_acc: 0.8987 \n",
      "\u001B[1A\u001B[2K  75/387 34.33s - train_loss: 0.2510 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  76/387 34.82s - train_loss: 0.2510 train_acc: 0.8989 \n",
      "\u001B[1A\u001B[2K  77/387 35.31s - train_loss: 0.2513 train_acc: 0.8988 \n",
      "\u001B[1A\u001B[2K  78/387 35.80s - train_loss: 0.2520 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  79/387 36.30s - train_loss: 0.2520 train_acc: 0.8987 \n",
      "\u001B[1A\u001B[2K  80/387 36.80s - train_loss: 0.2517 train_acc: 0.8988 \n",
      "\u001B[1A\u001B[2K  81/387 37.32s - train_loss: 0.2513 train_acc: 0.8989 \n",
      "\u001B[1A\u001B[2K  82/387 37.85s - train_loss: 0.2510 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  83/387 38.38s - train_loss: 0.2511 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  84/387 38.92s - train_loss: 0.2513 train_acc: 0.8991 \n",
      "\u001B[1A\u001B[2K  85/387 39.45s - train_loss: 0.2508 train_acc: 0.8993 \n",
      "\u001B[1A\u001B[2K  86/387 39.99s - train_loss: 0.2512 train_acc: 0.8992 \n",
      "\u001B[1A\u001B[2K  87/387 40.52s - train_loss: 0.2512 train_acc: 0.8991 \n",
      "\u001B[1A\u001B[2K  88/387 41.05s - train_loss: 0.2511 train_acc: 0.8992 \n",
      "\u001B[1A\u001B[2K  89/387 41.58s - train_loss: 0.2511 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  90/387 42.11s - train_loss: 0.2511 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  91/387 42.64s - train_loss: 0.2512 train_acc: 0.8989 \n",
      "\u001B[1A\u001B[2K  92/387 43.18s - train_loss: 0.2514 train_acc: 0.8987 \n",
      "\u001B[1A\u001B[2K  93/387 43.73s - train_loss: 0.2517 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  94/387 44.30s - train_loss: 0.2514 train_acc: 0.8987 \n",
      "\u001B[1A\u001B[2K  95/387 44.87s - train_loss: 0.2512 train_acc: 0.8988 \n",
      "\u001B[1A\u001B[2K  96/387 45.43s - train_loss: 0.2512 train_acc: 0.8987 \n",
      "\u001B[1A\u001B[2K  97/387 46.00s - train_loss: 0.2515 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  98/387 46.57s - train_loss: 0.2514 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  99/387 47.14s - train_loss: 0.2516 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  100/387 47.71s - train_loss: 0.2514 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  101/387 48.29s - train_loss: 0.2509 train_acc: 0.8987 \n",
      "\u001B[1A\u001B[2K  102/387 48.87s - train_loss: 0.2508 train_acc: 0.8988 \n",
      "\u001B[1A\u001B[2K  103/387 49.46s - train_loss: 0.2510 train_acc: 0.8988 \n",
      "\u001B[1A\u001B[2K  104/387 50.07s - train_loss: 0.2514 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  105/387 50.69s - train_loss: 0.2511 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  106/387 51.30s - train_loss: 0.2511 train_acc: 0.8987 \n",
      "\u001B[1A\u001B[2K  107/387 51.91s - train_loss: 0.2511 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  108/387 52.52s - train_loss: 0.2510 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  109/387 53.12s - train_loss: 0.2509 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  110/387 53.73s - train_loss: 0.2507 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  111/387 54.34s - train_loss: 0.2507 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  112/387 54.96s - train_loss: 0.2509 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  113/387 55.59s - train_loss: 0.2508 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  114/387 56.25s - train_loss: 0.2514 train_acc: 0.8981 \n",
      "\u001B[1A\u001B[2K  115/387 56.92s - train_loss: 0.2514 train_acc: 0.8982 \n",
      "\u001B[1A\u001B[2K  116/387 57.61s - train_loss: 0.2513 train_acc: 0.8982 \n",
      "\u001B[1A\u001B[2K  117/387 58.28s - train_loss: 0.2513 train_acc: 0.8982 \n",
      "\u001B[1A\u001B[2K  118/387 58.94s - train_loss: 0.2511 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  119/387 59.58s - train_loss: 0.2509 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  120/387 60.21s - train_loss: 0.2512 train_acc: 0.8982 \n",
      "\u001B[1A\u001B[2K  121/387 60.84s - train_loss: 0.2513 train_acc: 0.8982 \n",
      "\u001B[1A\u001B[2K  122/387 61.48s - train_loss: 0.2510 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  123/387 62.11s - train_loss: 0.2509 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  124/387 62.77s - train_loss: 0.2507 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  125/387 63.46s - train_loss: 0.2508 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  126/387 64.18s - train_loss: 0.2509 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  127/387 64.91s - train_loss: 0.2506 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  128/387 65.62s - train_loss: 0.2506 train_acc: 0.8981 \n",
      "\u001B[1A\u001B[2K  129/387 66.30s - train_loss: 0.2505 train_acc: 0.8981 \n",
      "\u001B[1A\u001B[2K  130/387 66.97s - train_loss: 0.2501 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  131/387 67.62s - train_loss: 0.2500 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  132/387 68.28s - train_loss: 0.2500 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  133/387 68.94s - train_loss: 0.2499 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  134/387 69.63s - train_loss: 0.2500 train_acc: 0.8983 \n",
      "\u001B[1A\u001B[2K  135/387 70.34s - train_loss: 0.2499 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  136/387 71.04s - train_loss: 0.2498 train_acc: 0.8984 \n",
      "\u001B[1A\u001B[2K  137/387 71.75s - train_loss: 0.2499 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  138/387 72.45s - train_loss: 0.2496 train_acc: 0.8985 \n",
      "\u001B[1A\u001B[2K  139/387 73.16s - train_loss: 0.2494 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  140/387 73.85s - train_loss: 0.2496 train_acc: 0.8986 \n",
      "\u001B[1A\u001B[2K  141/387 74.54s - train_loss: 0.2495 train_acc: 0.8987 \n",
      "\u001B[1A\u001B[2K  142/387 75.22s - train_loss: 0.2492 train_acc: 0.8988 \n",
      "\u001B[1A\u001B[2K  143/387 75.92s - train_loss: 0.2493 train_acc: 0.8988 \n",
      "\u001B[1A\u001B[2K  144/387 76.65s - train_loss: 0.2493 train_acc: 0.8988 \n",
      "\u001B[1A\u001B[2K  145/387 77.37s - train_loss: 0.2492 train_acc: 0.8989 \n",
      "\u001B[1A\u001B[2K  146/387 78.08s - train_loss: 0.2490 train_acc: 0.8989 \n",
      "\u001B[1A\u001B[2K  147/387 78.77s - train_loss: 0.2491 train_acc: 0.8988 \n",
      "\u001B[1A\u001B[2K  148/387 79.46s - train_loss: 0.2491 train_acc: 0.8988 \n",
      "\u001B[1A\u001B[2K  149/387 80.15s - train_loss: 0.2490 train_acc: 0.8988 \n",
      "\u001B[1A\u001B[2K  150/387 80.85s - train_loss: 0.2489 train_acc: 0.8989 \n",
      "\u001B[1A\u001B[2K  151/387 81.56s - train_loss: 0.2487 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  152/387 82.29s - train_loss: 0.2486 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  153/387 83.03s - train_loss: 0.2485 train_acc: 0.8991 \n",
      "\u001B[1A\u001B[2K  154/387 83.78s - train_loss: 0.2486 train_acc: 0.8990 \n",
      "\u001B[1A\u001B[2K  155/387 84.50s - train_loss: 0.2486 train_acc: 0.8991 \n",
      "\u001B[1A\u001B[2K  156/387 85.20s - train_loss: 0.2485 train_acc: 0.8991 \n",
      "\u001B[1A\u001B[2K  157/387 85.87s - train_loss: 0.2485 train_acc: 0.8991 \n",
      "\u001B[1A\u001B[2K  158/387 86.55s - train_loss: 0.2486 train_acc: 0.8992 \n",
      "\u001B[1A\u001B[2K  159/387 87.22s - train_loss: 0.2486 train_acc: 0.8992 \n",
      "\u001B[1A\u001B[2K  160/387 87.90s - train_loss: 0.2485 train_acc: 0.8992 \n",
      "\u001B[1A\u001B[2K  161/387 88.60s - train_loss: 0.2484 train_acc: 0.8993 \n",
      "\u001B[1A\u001B[2K  162/387 89.35s - train_loss: 0.2482 train_acc: 0.8994 \n",
      "\u001B[1A\u001B[2K  163/387 90.11s - train_loss: 0.2481 train_acc: 0.8995 \n",
      "\u001B[1A\u001B[2K  164/387 90.88s - train_loss: 0.2479 train_acc: 0.8996 \n",
      "\u001B[1A\u001B[2K  165/387 91.61s - train_loss: 0.2480 train_acc: 0.8995 \n",
      "\u001B[1A\u001B[2K  166/387 92.31s - train_loss: 0.2479 train_acc: 0.8995 \n",
      "\u001B[1A\u001B[2K  167/387 92.99s - train_loss: 0.2480 train_acc: 0.8993 \n",
      "\u001B[1A\u001B[2K  168/387 93.67s - train_loss: 0.2481 train_acc: 0.8992 \n",
      "\u001B[1A\u001B[2K  169/387 94.35s - train_loss: 0.2478 train_acc: 0.8994 \n",
      "\u001B[1A\u001B[2K  170/387 95.02s - train_loss: 0.2475 train_acc: 0.8995 \n",
      "\u001B[1A\u001B[2K  171/387 95.72s - train_loss: 0.2474 train_acc: 0.8996 \n",
      "\u001B[1A\u001B[2K  172/387 96.46s - train_loss: 0.2477 train_acc: 0.8995 \n",
      "\u001B[1A\u001B[2K  173/387 97.23s - train_loss: 0.2473 train_acc: 0.8996 \n",
      "\u001B[1A\u001B[2K  174/387 98.01s - train_loss: 0.2473 train_acc: 0.8996 \n",
      "\u001B[1A\u001B[2K  175/387 98.74s - train_loss: 0.2474 train_acc: 0.8996 \n",
      "\u001B[1A\u001B[2K  176/387 99.45s - train_loss: 0.2475 train_acc: 0.8995 \n",
      "\u001B[1A\u001B[2K  177/387 100.13s - train_loss: 0.2475 train_acc: 0.8995 \n",
      "\u001B[1A\u001B[2K  178/387 100.80s - train_loss: 0.2477 train_acc: 0.8994 \n",
      "\u001B[1A\u001B[2K  179/387 101.47s - train_loss: 0.2474 train_acc: 0.8995 \n",
      "\u001B[1A\u001B[2K  180/387 102.15s - train_loss: 0.2473 train_acc: 0.8996 \n",
      "\u001B[1A\u001B[2K  181/387 102.85s - train_loss: 0.2473 train_acc: 0.8996 \n",
      "\u001B[1A\u001B[2K  182/387 103.58s - train_loss: 0.2471 train_acc: 0.8997 \n",
      "\u001B[1A\u001B[2K  183/387 104.30s - train_loss: 0.2473 train_acc: 0.8996 \n",
      "\u001B[1A\u001B[2K  184/387 105.01s - train_loss: 0.2473 train_acc: 0.8996 \n",
      "\u001B[1A\u001B[2K  185/387 105.73s - train_loss: 0.2472 train_acc: 0.8997 \n",
      "\u001B[1A\u001B[2K  186/387 106.43s - train_loss: 0.2472 train_acc: 0.8997 \n",
      "\u001B[1A\u001B[2K  187/387 107.12s - train_loss: 0.2472 train_acc: 0.8996 \n",
      "\u001B[1A\u001B[2K  188/387 107.81s - train_loss: 0.2474 train_acc: 0.8996 \n",
      "\u001B[1A\u001B[2K  189/387 108.49s - train_loss: 0.2470 train_acc: 0.8998 \n",
      "\u001B[1A\u001B[2K  190/387 109.19s - train_loss: 0.2469 train_acc: 0.8999 \n",
      "\u001B[1A\u001B[2K  191/387 109.90s - train_loss: 0.2470 train_acc: 0.8999 \n",
      "\u001B[1A\u001B[2K  192/387 110.62s - train_loss: 0.2469 train_acc: 0.8999 \n",
      "\u001B[1A\u001B[2K  193/387 111.31s - train_loss: 0.2469 train_acc: 0.8999 \n",
      "\u001B[1A\u001B[2K  194/387 112.00s - train_loss: 0.2467 train_acc: 0.8999 \n",
      "\u001B[1A\u001B[2K  195/387 112.67s - train_loss: 0.2468 train_acc: 0.8999 \n",
      "\u001B[1A\u001B[2K  196/387 113.35s - train_loss: 0.2469 train_acc: 0.8999 \n",
      "\u001B[1A\u001B[2K  197/387 114.03s - train_loss: 0.2470 train_acc: 0.8999 \n",
      "\u001B[1A\u001B[2K  198/387 114.73s - train_loss: 0.2469 train_acc: 0.8999 \n",
      "\u001B[1A\u001B[2K  199/387 115.44s - train_loss: 0.2470 train_acc: 0.9000 \n",
      "\u001B[1A\u001B[2K  200/387 116.17s - train_loss: 0.2470 train_acc: 0.9000 \n",
      "\u001B[1A\u001B[2K  201/387 116.89s - train_loss: 0.2469 train_acc: 0.9001 \n",
      "\u001B[1A\u001B[2K  202/387 117.60s - train_loss: 0.2468 train_acc: 0.9001 \n",
      "\u001B[1A\u001B[2K  203/387 118.29s - train_loss: 0.2466 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  204/387 118.96s - train_loss: 0.2466 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  205/387 119.63s - train_loss: 0.2465 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  206/387 120.29s - train_loss: 0.2462 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  207/387 120.95s - train_loss: 0.2463 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  208/387 121.64s - train_loss: 0.2463 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  209/387 122.36s - train_loss: 0.2461 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  210/387 123.10s - train_loss: 0.2462 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  211/387 123.85s - train_loss: 0.2460 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  212/387 124.57s - train_loss: 0.2461 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  213/387 125.25s - train_loss: 0.2461 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  214/387 125.92s - train_loss: 0.2464 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  215/387 126.58s - train_loss: 0.2465 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  216/387 127.23s - train_loss: 0.2465 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  217/387 127.88s - train_loss: 0.2466 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  218/387 128.54s - train_loss: 0.2468 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  219/387 129.23s - train_loss: 0.2466 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  220/387 129.95s - train_loss: 0.2466 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  221/387 130.67s - train_loss: 0.2466 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  222/387 131.39s - train_loss: 0.2467 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  223/387 132.09s - train_loss: 0.2467 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  224/387 132.77s - train_loss: 0.2466 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  225/387 133.44s - train_loss: 0.2465 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  226/387 134.10s - train_loss: 0.2465 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  227/387 134.76s - train_loss: 0.2465 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  228/387 135.43s - train_loss: 0.2464 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  229/387 136.12s - train_loss: 0.2467 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  230/387 136.82s - train_loss: 0.2466 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  231/387 137.51s - train_loss: 0.2466 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  232/387 138.19s - train_loss: 0.2466 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  233/387 138.86s - train_loss: 0.2466 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  234/387 139.54s - train_loss: 0.2465 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  235/387 140.21s - train_loss: 0.2463 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  236/387 140.89s - train_loss: 0.2465 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  237/387 141.57s - train_loss: 0.2464 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  238/387 142.27s - train_loss: 0.2464 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  239/387 142.96s - train_loss: 0.2465 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  240/387 143.66s - train_loss: 0.2465 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  241/387 144.33s - train_loss: 0.2464 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  242/387 144.99s - train_loss: 0.2466 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  243/387 145.63s - train_loss: 0.2465 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  244/387 146.28s - train_loss: 0.2466 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  245/387 146.92s - train_loss: 0.2466 train_acc: 0.9002 \n",
      "\u001B[1A\u001B[2K  246/387 147.57s - train_loss: 0.2466 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  247/387 148.25s - train_loss: 0.2468 train_acc: 0.9002 \n",
      "\u001B[1A\u001B[2K  248/387 148.97s - train_loss: 0.2466 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  249/387 149.71s - train_loss: 0.2466 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  250/387 150.43s - train_loss: 0.2464 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  251/387 151.13s - train_loss: 0.2464 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  252/387 151.80s - train_loss: 0.2463 train_acc: 0.9003 \n",
      "\u001B[1A\u001B[2K  253/387 152.45s - train_loss: 0.2462 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  254/387 153.09s - train_loss: 0.2462 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  255/387 153.73s - train_loss: 0.2461 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  256/387 154.38s - train_loss: 0.2461 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  257/387 155.04s - train_loss: 0.2460 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  258/387 155.72s - train_loss: 0.2459 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  259/387 156.41s - train_loss: 0.2461 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  260/387 157.11s - train_loss: 0.2461 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  261/387 157.80s - train_loss: 0.2459 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  262/387 158.48s - train_loss: 0.2460 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  263/387 159.15s - train_loss: 0.2459 train_acc: 0.9005 \n",
      "\u001B[1A\u001B[2K  264/387 159.81s - train_loss: 0.2458 train_acc: 0.9006 \n",
      "\u001B[1A\u001B[2K  265/387 160.46s - train_loss: 0.2457 train_acc: 0.9006 \n",
      "\u001B[1A\u001B[2K  266/387 161.11s - train_loss: 0.2457 train_acc: 0.9006 \n",
      "\u001B[1A\u001B[2K  267/387 161.77s - train_loss: 0.2457 train_acc: 0.9006 \n",
      "\u001B[1A\u001B[2K  268/387 162.45s - train_loss: 0.2456 train_acc: 0.9006 \n",
      "\u001B[1A\u001B[2K  269/387 163.13s - train_loss: 0.2456 train_acc: 0.9007 \n",
      "\u001B[1A\u001B[2K  270/387 163.81s - train_loss: 0.2456 train_acc: 0.9007 \n",
      "\u001B[1A\u001B[2K  271/387 164.46s - train_loss: 0.2453 train_acc: 0.9008 \n",
      "\u001B[1A\u001B[2K  272/387 165.12s - train_loss: 0.2452 train_acc: 0.9009 \n",
      "\u001B[1A\u001B[2K  273/387 165.78s - train_loss: 0.2452 train_acc: 0.9010 \n",
      "\u001B[1A\u001B[2K  274/387 166.45s - train_loss: 0.2449 train_acc: 0.9011 \n",
      "\u001B[1A\u001B[2K  275/387 167.12s - train_loss: 0.2449 train_acc: 0.9010 \n",
      "\u001B[1A\u001B[2K  276/387 167.79s - train_loss: 0.2449 train_acc: 0.9011 \n",
      "\u001B[1A\u001B[2K  277/387 168.47s - train_loss: 0.2448 train_acc: 0.9010 \n",
      "\u001B[1A\u001B[2K  278/387 169.17s - train_loss: 0.2447 train_acc: 0.9011 \n",
      "\u001B[1A\u001B[2K  279/387 169.85s - train_loss: 0.2447 train_acc: 0.9011 \n",
      "\u001B[1A\u001B[2K  280/387 170.51s - train_loss: 0.2448 train_acc: 0.9010 \n",
      "\u001B[1A\u001B[2K  281/387 171.17s - train_loss: 0.2448 train_acc: 0.9010 \n",
      "\u001B[1A\u001B[2K  282/387 171.81s - train_loss: 0.2448 train_acc: 0.9010 \n",
      "\u001B[1A\u001B[2K  283/387 172.45s - train_loss: 0.2447 train_acc: 0.9010 \n",
      "\u001B[1A\u001B[2K  284/387 173.09s - train_loss: 0.2448 train_acc: 0.9010 \n",
      "\u001B[1A\u001B[2K  285/387 173.73s - train_loss: 0.2450 train_acc: 0.9010 \n",
      "\u001B[1A\u001B[2K  286/387 174.39s - train_loss: 0.2449 train_acc: 0.9011 \n",
      "\u001B[1A\u001B[2K  287/387 175.09s - train_loss: 0.2448 train_acc: 0.9011 \n",
      "\u001B[1A\u001B[2K  288/387 175.82s - train_loss: 0.2447 train_acc: 0.9012 \n",
      "\u001B[1A\u001B[2K  289/387 176.55s - train_loss: 0.2448 train_acc: 0.9012 \n",
      "\u001B[1A\u001B[2K  290/387 177.24s - train_loss: 0.2446 train_acc: 0.9012 \n",
      "\u001B[1A\u001B[2K  291/387 177.91s - train_loss: 0.2446 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  292/387 178.55s - train_loss: 0.2446 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  293/387 179.19s - train_loss: 0.2446 train_acc: 0.9012 \n",
      "\u001B[1A\u001B[2K  294/387 179.83s - train_loss: 0.2446 train_acc: 0.9012 \n",
      "\u001B[1A\u001B[2K  295/387 180.46s - train_loss: 0.2445 train_acc: 0.9012 \n",
      "\u001B[1A\u001B[2K  296/387 181.11s - train_loss: 0.2443 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  297/387 181.79s - train_loss: 0.2443 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  298/387 182.47s - train_loss: 0.2443 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  299/387 183.16s - train_loss: 0.2444 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  300/387 183.85s - train_loss: 0.2443 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  301/387 184.53s - train_loss: 0.2444 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  302/387 185.21s - train_loss: 0.2443 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  303/387 185.88s - train_loss: 0.2445 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  304/387 186.54s - train_loss: 0.2444 train_acc: 0.9014 \n",
      "\u001B[1A\u001B[2K  305/387 187.18s - train_loss: 0.2443 train_acc: 0.9014 \n",
      "\u001B[1A\u001B[2K  306/387 187.84s - train_loss: 0.2442 train_acc: 0.9014 \n",
      "\u001B[1A\u001B[2K  307/387 188.51s - train_loss: 0.2442 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  308/387 189.19s - train_loss: 0.2442 train_acc: 0.9014 \n",
      "\u001B[1A\u001B[2K  309/387 189.86s - train_loss: 0.2443 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  310/387 190.52s - train_loss: 0.2443 train_acc: 0.9013 \n",
      "\u001B[1A\u001B[2K  311/387 191.17s - train_loss: 0.2442 train_acc: 0.9014 \n",
      "\u001B[1A\u001B[2K  312/387 191.82s - train_loss: 0.2441 train_acc: 0.9014 \n",
      "\u001B[1A\u001B[2K  313/387 192.46s - train_loss: 0.2442 train_acc: 0.9014 \n",
      "\u001B[1A\u001B[2K  314/387 193.10s - train_loss: 0.2441 train_acc: 0.9015 \n",
      "\u001B[1A\u001B[2K  315/387 193.76s - train_loss: 0.2440 train_acc: 0.9015 \n",
      "\u001B[1A\u001B[2K  316/387 194.44s - train_loss: 0.2439 train_acc: 0.9015 \n",
      "\u001B[1A\u001B[2K  317/387 195.15s - train_loss: 0.2438 train_acc: 0.9016 \n",
      "\u001B[1A\u001B[2K  318/387 195.88s - train_loss: 0.2437 train_acc: 0.9016 \n",
      "\u001B[1A\u001B[2K  319/387 196.57s - train_loss: 0.2436 train_acc: 0.9016 \n",
      "\u001B[1A\u001B[2K  320/387 197.24s - train_loss: 0.2434 train_acc: 0.9018 \n",
      "\u001B[1A\u001B[2K  321/387 197.89s - train_loss: 0.2433 train_acc: 0.9018 \n",
      "\u001B[1A\u001B[2K  322/387 198.54s - train_loss: 0.2432 train_acc: 0.9019 \n",
      "\u001B[1A\u001B[2K  323/387 199.18s - train_loss: 0.2432 train_acc: 0.9019 \n",
      "\u001B[1A\u001B[2K  324/387 199.81s - train_loss: 0.2432 train_acc: 0.9019 \n",
      "\u001B[1A\u001B[2K  325/387 200.46s - train_loss: 0.2432 train_acc: 0.9020 \n",
      "\u001B[1A\u001B[2K  326/387 201.12s - train_loss: 0.2432 train_acc: 0.9019 \n",
      "\u001B[1A\u001B[2K  327/387 201.81s - train_loss: 0.2430 train_acc: 0.9020 \n",
      "\u001B[1A\u001B[2K  328/387 202.50s - train_loss: 0.2430 train_acc: 0.9020 \n",
      "\u001B[1A\u001B[2K  329/387 203.19s - train_loss: 0.2430 train_acc: 0.9021 \n",
      "\u001B[1A\u001B[2K  330/387 203.87s - train_loss: 0.2429 train_acc: 0.9021 \n",
      "\u001B[1A\u001B[2K  331/387 204.55s - train_loss: 0.2427 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  332/387 205.21s - train_loss: 0.2428 train_acc: 0.9021 \n",
      "\u001B[1A\u001B[2K  333/387 205.86s - train_loss: 0.2428 train_acc: 0.9021 \n",
      "\u001B[1A\u001B[2K  334/387 206.51s - train_loss: 0.2428 train_acc: 0.9021 \n",
      "\u001B[1A\u001B[2K  335/387 207.16s - train_loss: 0.2428 train_acc: 0.9021 \n",
      "\u001B[1A\u001B[2K  336/387 207.83s - train_loss: 0.2427 train_acc: 0.9021 \n",
      "\u001B[1A\u001B[2K  337/387 208.51s - train_loss: 0.2427 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  338/387 209.19s - train_loss: 0.2428 train_acc: 0.9021 \n",
      "\u001B[1A\u001B[2K  339/387 209.85s - train_loss: 0.2427 train_acc: 0.9021 \n",
      "\u001B[1A\u001B[2K  340/387 210.50s - train_loss: 0.2427 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  341/387 211.14s - train_loss: 0.2426 train_acc: 0.9021 \n",
      "\u001B[1A\u001B[2K  342/387 211.78s - train_loss: 0.2426 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  343/387 212.42s - train_loss: 0.2425 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  344/387 213.08s - train_loss: 0.2426 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  345/387 213.76s - train_loss: 0.2425 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  346/387 214.45s - train_loss: 0.2423 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  347/387 215.16s - train_loss: 0.2425 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  348/387 215.85s - train_loss: 0.2424 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  349/387 216.51s - train_loss: 0.2425 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  350/387 217.17s - train_loss: 0.2426 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  351/387 217.81s - train_loss: 0.2426 train_acc: 0.9021 \n",
      "\u001B[1A\u001B[2K  352/387 218.45s - train_loss: 0.2426 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  353/387 219.08s - train_loss: 0.2426 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  354/387 219.72s - train_loss: 0.2425 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  355/387 220.37s - train_loss: 0.2424 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  356/387 221.04s - train_loss: 0.2424 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  357/387 221.73s - train_loss: 0.2423 train_acc: 0.9023 \n",
      "\u001B[1A\u001B[2K  358/387 222.43s - train_loss: 0.2423 train_acc: 0.9023 \n",
      "\u001B[1A\u001B[2K  359/387 223.13s - train_loss: 0.2424 train_acc: 0.9023 \n",
      "\u001B[1A\u001B[2K  360/387 223.81s - train_loss: 0.2424 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  361/387 224.47s - train_loss: 0.2423 train_acc: 0.9022 \n",
      "\u001B[1A\u001B[2K  362/387 225.12s - train_loss: 0.2423 train_acc: 0.9023 \n",
      "\u001B[1A\u001B[2K  363/387 225.76s - train_loss: 0.2422 train_acc: 0.9023 \n",
      "\u001B[1A\u001B[2K  364/387 226.39s - train_loss: 0.2421 train_acc: 0.9023 \n",
      "\u001B[1A\u001B[2K  365/387 227.04s - train_loss: 0.2422 train_acc: 0.9023 \n",
      "\u001B[1A\u001B[2K  366/387 227.71s - train_loss: 0.2422 train_acc: 0.9023 \n",
      "\u001B[1A\u001B[2K  367/387 228.39s - train_loss: 0.2423 train_acc: 0.9023 \n",
      "\u001B[1A\u001B[2K  368/387 229.06s - train_loss: 0.2422 train_acc: 0.9024 \n",
      "\u001B[1A\u001B[2K  369/387 229.72s - train_loss: 0.2421 train_acc: 0.9024 \n",
      "\u001B[1A\u001B[2K  370/387 230.37s - train_loss: 0.2420 train_acc: 0.9024 \n",
      "\u001B[1A\u001B[2K  371/387 231.02s - train_loss: 0.2421 train_acc: 0.9024 \n",
      "\u001B[1A\u001B[2K  372/387 231.68s - train_loss: 0.2421 train_acc: 0.9024 \n",
      "\u001B[1A\u001B[2K  373/387 232.34s - train_loss: 0.2422 train_acc: 0.9024 \n",
      "\u001B[1A\u001B[2K  374/387 233.01s - train_loss: 0.2422 train_acc: 0.9024 \n",
      "\u001B[1A\u001B[2K  375/387 233.69s - train_loss: 0.2422 train_acc: 0.9024 \n",
      "\u001B[1A\u001B[2K  376/387 234.37s - train_loss: 0.2422 train_acc: 0.9024 \n",
      "\u001B[1A\u001B[2K  377/387 235.06s - train_loss: 0.2421 train_acc: 0.9024 \n",
      "\u001B[1A\u001B[2K  378/387 235.73s - train_loss: 0.2420 train_acc: 0.9025 \n",
      "\u001B[1A\u001B[2K  379/387 236.40s - train_loss: 0.2420 train_acc: 0.9025 \n",
      "\u001B[1A\u001B[2K  380/387 237.04s - train_loss: 0.2420 train_acc: 0.9025 \n",
      "\u001B[1A\u001B[2K  381/387 237.66s - train_loss: 0.2420 train_acc: 0.9025 \n",
      "\u001B[1A\u001B[2K  382/387 238.26s - train_loss: 0.2419 train_acc: 0.9026 \n",
      "\u001B[1A\u001B[2K  383/387 238.84s - train_loss: 0.2419 train_acc: 0.9026 \n",
      "\u001B[1A\u001B[2K  384/387 239.41s - train_loss: 0.2420 train_acc: 0.9025 \n",
      "\u001B[1A\u001B[2K  385/387 239.98s - train_loss: 0.2419 train_acc: 0.9026 \n",
      "\u001B[1A\u001B[2K  386/387 240.57s - train_loss: 0.2419 train_acc: 0.9026 \n",
      "\u001B[1A\u001B[2K  387/387 241.04s - train_loss: 0.2419 train_acc: 0.9026 \n",
      "\u001B[1A\u001B[2K  288.71s - train_loss: 0.2419 train_acc: 0.9026 std_val_loss: 0.0005 std_val_acc: 0.9015 \n",
      "3/3 epoch:\n",
      "\n",
      "\u001B[1A\u001B[2K  1/387 3.00s - train_loss: 0.2484 train_acc: 0.8945 \n",
      "\u001B[1A\u001B[2K  2/387 3.36s - train_loss: 0.2434 train_acc: 0.8975 \n",
      "\u001B[1A\u001B[2K  3/387 3.73s - train_loss: 0.2289 train_acc: 0.9049 \n",
      "\u001B[1A\u001B[2K  4/387 4.10s - train_loss: 0.2360 train_acc: 0.9004 \n",
      "\u001B[1A\u001B[2K  5/387 4.46s - train_loss: 0.2374 train_acc: 0.9016 \n",
      "\u001B[1A\u001B[2K  6/387 4.83s - train_loss: 0.2333 train_acc: 0.9046 \n",
      "\u001B[1A\u001B[2K  7/387 5.20s - train_loss: 0.2334 train_acc: 0.9057 \n",
      "\u001B[1A\u001B[2K  8/387 5.57s - train_loss: 0.2376 train_acc: 0.9055 \n",
      "\u001B[1A\u001B[2K  9/387 5.94s - train_loss: 0.2362 train_acc: 0.9058 \n",
      "\u001B[1A\u001B[2K  10/387 6.31s - train_loss: 0.2312 train_acc: 0.9086 \n",
      "\u001B[1A\u001B[2K  11/387 6.67s - train_loss: 0.2308 train_acc: 0.9089 \n",
      "\u001B[1A\u001B[2K  12/387 7.04s - train_loss: 0.2286 train_acc: 0.9102 \n",
      "\u001B[1A\u001B[2K  13/387 7.41s - train_loss: 0.2260 train_acc: 0.9112 \n",
      "\u001B[1A\u001B[2K  14/387 7.78s - train_loss: 0.2229 train_acc: 0.9121 \n",
      "\u001B[1A\u001B[2K  15/387 8.14s - train_loss: 0.2212 train_acc: 0.9134 \n",
      "\u001B[1A\u001B[2K  16/387 8.51s - train_loss: 0.2234 train_acc: 0.9132 \n",
      "\u001B[1A\u001B[2K  17/387 8.88s - train_loss: 0.2211 train_acc: 0.9145 \n",
      "\u001B[1A\u001B[2K  18/387 9.24s - train_loss: 0.2199 train_acc: 0.9153 \n",
      "\u001B[1A\u001B[2K  19/387 9.61s - train_loss: 0.2188 train_acc: 0.9158 \n",
      "\u001B[1A\u001B[2K  20/387 9.98s - train_loss: 0.2165 train_acc: 0.9165 \n",
      "\u001B[1A\u001B[2K  21/387 10.35s - train_loss: 0.2173 train_acc: 0.9153 \n",
      "\u001B[1A\u001B[2K  22/387 10.72s - train_loss: 0.2148 train_acc: 0.9165 \n",
      "\u001B[1A\u001B[2K  23/387 11.09s - train_loss: 0.2139 train_acc: 0.9169 \n",
      "\u001B[1A\u001B[2K  24/387 11.47s - train_loss: 0.2131 train_acc: 0.9172 \n",
      "\u001B[1A\u001B[2K  25/387 11.85s - train_loss: 0.2128 train_acc: 0.9170 \n",
      "\u001B[1A\u001B[2K  26/387 12.23s - train_loss: 0.2131 train_acc: 0.9169 \n",
      "\u001B[1A\u001B[2K  27/387 12.62s - train_loss: 0.2131 train_acc: 0.9167 \n",
      "\u001B[1A\u001B[2K  28/387 13.00s - train_loss: 0.2118 train_acc: 0.9178 \n",
      "\u001B[1A\u001B[2K  29/387 13.39s - train_loss: 0.2112 train_acc: 0.9180 \n",
      "\u001B[1A\u001B[2K  30/387 13.78s - train_loss: 0.2116 train_acc: 0.9176 \n",
      "\u001B[1A\u001B[2K  31/387 14.17s - train_loss: 0.2111 train_acc: 0.9177 \n",
      "\u001B[1A\u001B[2K  32/387 14.57s - train_loss: 0.2101 train_acc: 0.9184 \n",
      "\u001B[1A\u001B[2K  33/387 14.96s - train_loss: 0.2107 train_acc: 0.9181 \n",
      "\u001B[1A\u001B[2K  34/387 15.35s - train_loss: 0.2098 train_acc: 0.9183 \n",
      "\u001B[1A\u001B[2K  35/387 15.75s - train_loss: 0.2110 train_acc: 0.9177 \n",
      "\u001B[1A\u001B[2K  36/387 16.15s - train_loss: 0.2105 train_acc: 0.9175 \n",
      "\u001B[1A\u001B[2K  37/387 16.54s - train_loss: 0.2105 train_acc: 0.9173 \n",
      "\u001B[1A\u001B[2K  38/387 16.93s - train_loss: 0.2103 train_acc: 0.9177 \n",
      "\u001B[1A\u001B[2K  39/387 17.33s - train_loss: 0.2102 train_acc: 0.9177 \n",
      "\u001B[1A\u001B[2K  40/387 17.72s - train_loss: 0.2103 train_acc: 0.9177 \n",
      "\u001B[1A\u001B[2K  41/387 18.12s - train_loss: 0.2099 train_acc: 0.9180 \n",
      "\u001B[1A\u001B[2K  42/387 18.53s - train_loss: 0.2096 train_acc: 0.9182 \n",
      "\u001B[1A\u001B[2K  43/387 18.93s - train_loss: 0.2092 train_acc: 0.9185 \n",
      "\u001B[1A\u001B[2K  44/387 19.34s - train_loss: 0.2081 train_acc: 0.9188 \n",
      "\u001B[1A\u001B[2K  45/387 19.74s - train_loss: 0.2075 train_acc: 0.9189 \n",
      "\u001B[1A\u001B[2K  46/387 20.15s - train_loss: 0.2070 train_acc: 0.9192 \n",
      "\u001B[1A\u001B[2K  47/387 20.56s - train_loss: 0.2076 train_acc: 0.9189 \n",
      "\u001B[1A\u001B[2K  48/387 20.97s - train_loss: 0.2079 train_acc: 0.9188 \n",
      "\u001B[1A\u001B[2K  49/387 21.38s - train_loss: 0.2080 train_acc: 0.9184 \n",
      "\u001B[1A\u001B[2K  50/387 21.79s - train_loss: 0.2083 train_acc: 0.9184 \n",
      "\u001B[1A\u001B[2K  51/387 22.19s - train_loss: 0.2075 train_acc: 0.9187 \n",
      "\u001B[1A\u001B[2K  52/387 22.60s - train_loss: 0.2069 train_acc: 0.9191 \n",
      "\u001B[1A\u001B[2K  53/387 23.01s - train_loss: 0.2076 train_acc: 0.9189 \n",
      "\u001B[1A\u001B[2K  54/387 23.42s - train_loss: 0.2076 train_acc: 0.9189 \n",
      "\u001B[1A\u001B[2K  55/387 23.83s - train_loss: 0.2075 train_acc: 0.9191 \n",
      "\u001B[1A\u001B[2K  56/387 24.24s - train_loss: 0.2080 train_acc: 0.9190 \n",
      "\u001B[1A\u001B[2K  57/387 24.66s - train_loss: 0.2076 train_acc: 0.9191 \n",
      "\u001B[1A\u001B[2K  58/387 25.08s - train_loss: 0.2074 train_acc: 0.9194 \n",
      "\u001B[1A\u001B[2K  59/387 25.50s - train_loss: 0.2084 train_acc: 0.9189 \n",
      "\u001B[1A\u001B[2K  60/387 25.92s - train_loss: 0.2082 train_acc: 0.9192 \n",
      "\u001B[1A\u001B[2K  61/387 26.35s - train_loss: 0.2081 train_acc: 0.9192 \n",
      "\u001B[1A\u001B[2K  62/387 26.77s - train_loss: 0.2082 train_acc: 0.9191 \n",
      "\u001B[1A\u001B[2K  63/387 27.20s - train_loss: 0.2086 train_acc: 0.9191 \n",
      "\u001B[1A\u001B[2K  64/387 27.63s - train_loss: 0.2085 train_acc: 0.9194 \n",
      "\u001B[1A\u001B[2K  65/387 28.06s - train_loss: 0.2083 train_acc: 0.9194 \n",
      "\u001B[1A\u001B[2K  66/387 28.49s - train_loss: 0.2076 train_acc: 0.9196 \n",
      "\u001B[1A\u001B[2K  67/387 28.93s - train_loss: 0.2077 train_acc: 0.9196 \n",
      "\u001B[1A\u001B[2K  68/387 29.36s - train_loss: 0.2074 train_acc: 0.9197 \n",
      "\u001B[1A\u001B[2K  69/387 29.80s - train_loss: 0.2075 train_acc: 0.9198 \n",
      "\u001B[1A\u001B[2K  70/387 30.23s - train_loss: 0.2074 train_acc: 0.9197 \n",
      "\u001B[1A\u001B[2K  71/387 30.68s - train_loss: 0.2074 train_acc: 0.9198 \n",
      "\u001B[1A\u001B[2K  72/387 31.13s - train_loss: 0.2078 train_acc: 0.9198 \n",
      "\u001B[1A\u001B[2K  73/387 31.59s - train_loss: 0.2078 train_acc: 0.9196 \n",
      "\u001B[1A\u001B[2K  74/387 32.05s - train_loss: 0.2079 train_acc: 0.9195 \n",
      "\u001B[1A\u001B[2K  75/387 32.52s - train_loss: 0.2078 train_acc: 0.9195 \n",
      "\u001B[1A\u001B[2K  76/387 32.98s - train_loss: 0.2075 train_acc: 0.9195 \n",
      "\u001B[1A\u001B[2K  77/387 33.45s - train_loss: 0.2077 train_acc: 0.9193 \n",
      "\u001B[1A\u001B[2K  78/387 33.93s - train_loss: 0.2073 train_acc: 0.9192 \n",
      "\u001B[1A\u001B[2K  79/387 34.39s - train_loss: 0.2071 train_acc: 0.9193 \n",
      "\u001B[1A\u001B[2K  80/387 34.86s - train_loss: 0.2068 train_acc: 0.9194 \n",
      "\u001B[1A\u001B[2K  81/387 35.34s - train_loss: 0.2070 train_acc: 0.9193 \n",
      "\u001B[1A\u001B[2K  82/387 35.81s - train_loss: 0.2067 train_acc: 0.9194 \n",
      "\u001B[1A\u001B[2K  83/387 36.28s - train_loss: 0.2065 train_acc: 0.9194 \n",
      "\u001B[1A\u001B[2K  84/387 36.76s - train_loss: 0.2064 train_acc: 0.9194 \n",
      "\u001B[1A\u001B[2K  85/387 37.25s - train_loss: 0.2067 train_acc: 0.9193 \n",
      "\u001B[1A\u001B[2K  86/387 37.74s - train_loss: 0.2065 train_acc: 0.9194 \n",
      "\u001B[1A\u001B[2K  87/387 38.25s - train_loss: 0.2062 train_acc: 0.9195 \n",
      "\u001B[1A\u001B[2K  88/387 38.75s - train_loss: 0.2062 train_acc: 0.9196 \n",
      "\u001B[1A\u001B[2K  89/387 39.26s - train_loss: 0.2062 train_acc: 0.9196 \n",
      "\u001B[1A\u001B[2K  90/387 39.77s - train_loss: 0.2059 train_acc: 0.9198 \n",
      "\u001B[1A\u001B[2K  91/387 40.29s - train_loss: 0.2060 train_acc: 0.9198 \n",
      "\u001B[1A\u001B[2K  92/387 40.81s - train_loss: 0.2064 train_acc: 0.9195 \n",
      "\u001B[1A\u001B[2K  93/387 41.33s - train_loss: 0.2064 train_acc: 0.9195 \n",
      "\u001B[1A\u001B[2K  94/387 41.86s - train_loss: 0.2064 train_acc: 0.9195 \n",
      "\u001B[1A\u001B[2K  95/387 42.38s - train_loss: 0.2063 train_acc: 0.9195 \n",
      "\u001B[1A\u001B[2K  96/387 42.91s - train_loss: 0.2062 train_acc: 0.9197 \n",
      "\u001B[1A\u001B[2K  97/387 43.46s - train_loss: 0.2060 train_acc: 0.9197 \n",
      "\u001B[1A\u001B[2K  98/387 44.01s - train_loss: 0.2059 train_acc: 0.9199 \n",
      "\u001B[1A\u001B[2K  99/387 44.57s - train_loss: 0.2059 train_acc: 0.9198 \n",
      "\u001B[1A\u001B[2K  100/387 45.12s - train_loss: 0.2061 train_acc: 0.9198 \n",
      "\u001B[1A\u001B[2K  101/387 45.66s - train_loss: 0.2060 train_acc: 0.9198 \n",
      "\u001B[1A\u001B[2K  102/387 46.20s - train_loss: 0.2058 train_acc: 0.9198 \n",
      "\u001B[1A\u001B[2K  103/387 46.74s - train_loss: 0.2056 train_acc: 0.9198 \n",
      "\u001B[1A\u001B[2K  104/387 47.28s - train_loss: 0.2055 train_acc: 0.9199 \n",
      "\u001B[1A\u001B[2K  105/387 47.82s - train_loss: 0.2053 train_acc: 0.9200 \n",
      "\u001B[1A\u001B[2K  106/387 48.35s - train_loss: 0.2053 train_acc: 0.9199 \n",
      "\u001B[1A\u001B[2K  107/387 48.90s - train_loss: 0.2050 train_acc: 0.9199 \n",
      "\u001B[1A\u001B[2K  108/387 49.46s - train_loss: 0.2046 train_acc: 0.9201 \n",
      "\u001B[1A\u001B[2K  109/387 50.05s - train_loss: 0.2044 train_acc: 0.9202 \n",
      "\u001B[1A\u001B[2K  110/387 50.66s - train_loss: 0.2042 train_acc: 0.9202 \n",
      "\u001B[1A\u001B[2K  111/387 51.26s - train_loss: 0.2040 train_acc: 0.9202 \n",
      "\u001B[1A\u001B[2K  112/387 51.87s - train_loss: 0.2040 train_acc: 0.9203 \n",
      "\u001B[1A\u001B[2K  113/387 52.47s - train_loss: 0.2042 train_acc: 0.9202 \n",
      "\u001B[1A\u001B[2K  114/387 53.07s - train_loss: 0.2046 train_acc: 0.9199 \n",
      "\u001B[1A\u001B[2K  115/387 53.66s - train_loss: 0.2044 train_acc: 0.9200 \n",
      "\u001B[1A\u001B[2K  116/387 54.26s - train_loss: 0.2042 train_acc: 0.9202 \n",
      "\u001B[1A\u001B[2K  117/387 54.85s - train_loss: 0.2043 train_acc: 0.9202 \n",
      "\u001B[1A\u001B[2K  118/387 55.46s - train_loss: 0.2041 train_acc: 0.9202 \n",
      "\u001B[1A\u001B[2K  119/387 56.09s - train_loss: 0.2038 train_acc: 0.9203 \n",
      "\u001B[1A\u001B[2K  120/387 56.74s - train_loss: 0.2039 train_acc: 0.9203 \n",
      "\u001B[1A\u001B[2K  121/387 57.40s - train_loss: 0.2037 train_acc: 0.9204 \n",
      "\u001B[1A\u001B[2K  122/387 58.06s - train_loss: 0.2033 train_acc: 0.9205 \n",
      "\u001B[1A\u001B[2K  123/387 58.70s - train_loss: 0.2036 train_acc: 0.9204 \n",
      "\u001B[1A\u001B[2K  124/387 59.34s - train_loss: 0.2037 train_acc: 0.9204 \n",
      "\u001B[1A\u001B[2K  125/387 59.97s - train_loss: 0.2035 train_acc: 0.9206 \n",
      "\u001B[1A\u001B[2K  126/387 60.60s - train_loss: 0.2034 train_acc: 0.9206 \n",
      "\u001B[1A\u001B[2K  127/387 61.22s - train_loss: 0.2034 train_acc: 0.9206 \n",
      "\u001B[1A\u001B[2K  128/387 61.86s - train_loss: 0.2034 train_acc: 0.9206 \n",
      "\u001B[1A\u001B[2K  129/387 62.53s - train_loss: 0.2034 train_acc: 0.9206 \n",
      "\u001B[1A\u001B[2K  130/387 63.22s - train_loss: 0.2033 train_acc: 0.9206 \n",
      "\u001B[1A\u001B[2K  131/387 63.91s - train_loss: 0.2036 train_acc: 0.9205 \n",
      "\u001B[1A\u001B[2K  132/387 64.59s - train_loss: 0.2035 train_acc: 0.9205 \n",
      "\u001B[1A\u001B[2K  133/387 65.26s - train_loss: 0.2035 train_acc: 0.9206 \n",
      "\u001B[1A\u001B[2K  134/387 65.92s - train_loss: 0.2030 train_acc: 0.9208 \n",
      "\u001B[1A\u001B[2K  135/387 66.57s - train_loss: 0.2030 train_acc: 0.9208 \n",
      "\u001B[1A\u001B[2K  136/387 67.23s - train_loss: 0.2027 train_acc: 0.9209 \n",
      "\u001B[1A\u001B[2K  137/387 67.88s - train_loss: 0.2028 train_acc: 0.9208 \n",
      "\u001B[1A\u001B[2K  138/387 68.57s - train_loss: 0.2029 train_acc: 0.9208 \n",
      "\u001B[1A\u001B[2K  139/387 69.29s - train_loss: 0.2029 train_acc: 0.9208 \n",
      "\u001B[1A\u001B[2K  140/387 70.01s - train_loss: 0.2028 train_acc: 0.9208 \n",
      "\u001B[1A\u001B[2K  141/387 70.72s - train_loss: 0.2027 train_acc: 0.9209 \n",
      "\u001B[1A\u001B[2K  142/387 71.40s - train_loss: 0.2028 train_acc: 0.9208 \n",
      "\u001B[1A\u001B[2K  143/387 72.08s - train_loss: 0.2027 train_acc: 0.9208 \n",
      "\u001B[1A\u001B[2K  144/387 72.74s - train_loss: 0.2027 train_acc: 0.9209 \n",
      "\u001B[1A\u001B[2K  145/387 73.40s - train_loss: 0.2025 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  146/387 74.06s - train_loss: 0.2026 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  147/387 74.75s - train_loss: 0.2024 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  148/387 75.57s - train_loss: 0.2023 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  149/387 77.24s - train_loss: 0.2023 train_acc: 0.9211 \n",
      "\u001B[1A\u001B[2K  150/387 78.56s - train_loss: 0.2022 train_acc: 0.9211 \n",
      "\u001B[1A\u001B[2K  151/387 79.10s - train_loss: 0.2023 train_acc: 0.9211 \n",
      "\u001B[1A\u001B[2K  152/387 79.66s - train_loss: 0.2023 train_acc: 0.9211 \n",
      "\u001B[1A\u001B[2K  153/387 80.25s - train_loss: 0.2024 train_acc: 0.9211 \n",
      "\u001B[1A\u001B[2K  154/387 80.85s - train_loss: 0.2021 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  155/387 81.47s - train_loss: 0.2021 train_acc: 0.9213 \n",
      "\u001B[1A\u001B[2K  156/387 82.09s - train_loss: 0.2021 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  157/387 82.72s - train_loss: 0.2021 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  158/387 83.37s - train_loss: 0.2019 train_acc: 0.9213 \n",
      "\u001B[1A\u001B[2K  159/387 84.03s - train_loss: 0.2020 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  160/387 84.72s - train_loss: 0.2021 train_acc: 0.9211 \n",
      "\u001B[1A\u001B[2K  161/387 85.46s - train_loss: 0.2020 train_acc: 0.9211 \n",
      "\u001B[1A\u001B[2K  162/387 86.18s - train_loss: 0.2019 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  163/387 86.90s - train_loss: 0.2019 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  164/387 87.59s - train_loss: 0.2022 train_acc: 0.9211 \n",
      "\u001B[1A\u001B[2K  165/387 88.26s - train_loss: 0.2022 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  166/387 88.94s - train_loss: 0.2022 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  167/387 89.61s - train_loss: 0.2021 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  168/387 90.28s - train_loss: 0.2022 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  169/387 90.98s - train_loss: 0.2023 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  170/387 91.71s - train_loss: 0.2023 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  171/387 92.45s - train_loss: 0.2023 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  172/387 93.17s - train_loss: 0.2022 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  173/387 93.86s - train_loss: 0.2021 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  174/387 94.54s - train_loss: 0.2019 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  175/387 95.22s - train_loss: 0.2018 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  176/387 95.88s - train_loss: 0.2018 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  177/387 96.55s - train_loss: 0.2018 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  178/387 97.24s - train_loss: 0.2018 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  179/387 97.97s - train_loss: 0.2017 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  180/387 98.71s - train_loss: 0.2019 train_acc: 0.9211 \n",
      "\u001B[1A\u001B[2K  181/387 99.43s - train_loss: 0.2021 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  182/387 100.14s - train_loss: 0.2024 train_acc: 0.9209 \n",
      "\u001B[1A\u001B[2K  183/387 100.83s - train_loss: 0.2024 train_acc: 0.9209 \n",
      "\u001B[1A\u001B[2K  184/387 101.50s - train_loss: 0.2025 train_acc: 0.9208 \n",
      "\u001B[1A\u001B[2K  185/387 102.17s - train_loss: 0.2024 train_acc: 0.9209 \n",
      "\u001B[1A\u001B[2K  186/387 102.83s - train_loss: 0.2025 train_acc: 0.9209 \n",
      "\u001B[1A\u001B[2K  187/387 103.52s - train_loss: 0.2026 train_acc: 0.9209 \n",
      "\u001B[1A\u001B[2K  188/387 104.25s - train_loss: 0.2024 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  189/387 105.00s - train_loss: 0.2024 train_acc: 0.9209 \n",
      "\u001B[1A\u001B[2K  190/387 105.71s - train_loss: 0.2023 train_acc: 0.9211 \n",
      "\u001B[1A\u001B[2K  191/387 106.41s - train_loss: 0.2021 train_acc: 0.9210 \n",
      "\u001B[1A\u001B[2K  192/387 107.09s - train_loss: 0.2018 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  193/387 107.77s - train_loss: 0.2016 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  194/387 108.44s - train_loss: 0.2017 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  195/387 109.10s - train_loss: 0.2017 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  196/387 109.80s - train_loss: 0.2016 train_acc: 0.9213 \n",
      "\u001B[1A\u001B[2K  197/387 110.52s - train_loss: 0.2014 train_acc: 0.9213 \n",
      "\u001B[1A\u001B[2K  198/387 111.29s - train_loss: 0.2015 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  199/387 112.10s - train_loss: 0.2017 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  200/387 112.86s - train_loss: 0.2016 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  201/387 113.58s - train_loss: 0.2016 train_acc: 0.9212 \n",
      "\u001B[1A\u001B[2K  202/387 114.26s - train_loss: 0.2014 train_acc: 0.9213 \n",
      "\u001B[1A\u001B[2K  203/387 114.92s - train_loss: 0.2013 train_acc: 0.9214 \n",
      "\u001B[1A\u001B[2K  204/387 115.56s - train_loss: 0.2011 train_acc: 0.9214 \n",
      "\u001B[1A\u001B[2K  205/387 116.23s - train_loss: 0.2011 train_acc: 0.9214 \n",
      "\u001B[1A\u001B[2K  206/387 116.91s - train_loss: 0.2009 train_acc: 0.9215 \n",
      "\u001B[1A\u001B[2K  207/387 117.60s - train_loss: 0.2009 train_acc: 0.9215 \n",
      "\u001B[1A\u001B[2K  208/387 118.29s - train_loss: 0.2009 train_acc: 0.9216 \n",
      "\u001B[1A\u001B[2K  209/387 118.96s - train_loss: 0.2007 train_acc: 0.9217 \n",
      "\u001B[1A\u001B[2K  210/387 119.63s - train_loss: 0.2011 train_acc: 0.9215 \n",
      "\u001B[1A\u001B[2K  211/387 120.29s - train_loss: 0.2010 train_acc: 0.9216 \n",
      "\u001B[1A\u001B[2K  212/387 120.95s - train_loss: 0.2010 train_acc: 0.9215 \n",
      "\u001B[1A\u001B[2K  213/387 121.61s - train_loss: 0.2010 train_acc: 0.9216 \n",
      "\u001B[1A\u001B[2K  214/387 122.28s - train_loss: 0.2008 train_acc: 0.9216 \n",
      "\u001B[1A\u001B[2K  215/387 122.97s - train_loss: 0.2008 train_acc: 0.9217 \n",
      "\u001B[1A\u001B[2K  216/387 123.70s - train_loss: 0.2010 train_acc: 0.9215 \n",
      "\u001B[1A\u001B[2K  217/387 124.42s - train_loss: 0.2009 train_acc: 0.9216 \n",
      "\u001B[1A\u001B[2K  218/387 125.11s - train_loss: 0.2011 train_acc: 0.9216 \n",
      "\u001B[1A\u001B[2K  219/387 125.79s - train_loss: 0.2012 train_acc: 0.9215 \n",
      "\u001B[1A\u001B[2K  220/387 126.47s - train_loss: 0.2010 train_acc: 0.9216 \n",
      "\u001B[1A\u001B[2K  221/387 127.13s - train_loss: 0.2009 train_acc: 0.9217 \n",
      "\u001B[1A\u001B[2K  222/387 127.78s - train_loss: 0.2007 train_acc: 0.9217 \n",
      "\u001B[1A\u001B[2K  223/387 128.43s - train_loss: 0.2006 train_acc: 0.9217 \n",
      "\u001B[1A\u001B[2K  224/387 129.11s - train_loss: 0.2007 train_acc: 0.9217 \n",
      "\u001B[1A\u001B[2K  225/387 129.82s - train_loss: 0.2007 train_acc: 0.9217 \n",
      "\u001B[1A\u001B[2K  226/387 130.53s - train_loss: 0.2007 train_acc: 0.9216 \n",
      "\u001B[1A\u001B[2K  227/387 131.23s - train_loss: 0.2006 train_acc: 0.9217 \n",
      "\u001B[1A\u001B[2K  228/387 131.91s - train_loss: 0.2006 train_acc: 0.9217 \n",
      "\u001B[1A\u001B[2K  229/387 132.58s - train_loss: 0.2006 train_acc: 0.9217 \n",
      "\u001B[1A\u001B[2K  230/387 133.24s - train_loss: 0.2006 train_acc: 0.9217 \n",
      "\u001B[1A\u001B[2K  231/387 133.89s - train_loss: 0.2007 train_acc: 0.9218 \n",
      "\u001B[1A\u001B[2K  232/387 134.54s - train_loss: 0.2006 train_acc: 0.9218 \n",
      "\u001B[1A\u001B[2K  233/387 135.20s - train_loss: 0.2005 train_acc: 0.9218 \n",
      "\u001B[1A\u001B[2K  234/387 135.90s - train_loss: 0.2005 train_acc: 0.9218 \n",
      "\u001B[1A\u001B[2K  235/387 136.61s - train_loss: 0.2004 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  236/387 137.30s - train_loss: 0.2003 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  237/387 137.98s - train_loss: 0.2003 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  238/387 138.65s - train_loss: 0.2003 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  239/387 139.30s - train_loss: 0.2003 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  240/387 139.95s - train_loss: 0.2002 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  241/387 140.59s - train_loss: 0.2003 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  242/387 141.23s - train_loss: 0.2002 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  243/387 141.91s - train_loss: 0.2001 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  244/387 142.61s - train_loss: 0.2000 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  245/387 143.30s - train_loss: 0.2000 train_acc: 0.9221 \n",
      "\u001B[1A\u001B[2K  246/387 143.98s - train_loss: 0.1998 train_acc: 0.9221 \n",
      "\u001B[1A\u001B[2K  247/387 144.64s - train_loss: 0.1999 train_acc: 0.9221 \n",
      "\u001B[1A\u001B[2K  248/387 145.30s - train_loss: 0.1998 train_acc: 0.9221 \n",
      "\u001B[1A\u001B[2K  249/387 145.94s - train_loss: 0.1999 train_acc: 0.9221 \n",
      "\u001B[1A\u001B[2K  250/387 146.57s - train_loss: 0.1999 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  251/387 147.21s - train_loss: 0.2001 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  252/387 147.86s - train_loss: 0.2001 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  253/387 148.55s - train_loss: 0.2002 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  254/387 149.25s - train_loss: 0.2002 train_acc: 0.9218 \n",
      "\u001B[1A\u001B[2K  255/387 149.93s - train_loss: 0.2001 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  256/387 150.59s - train_loss: 0.2002 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  257/387 151.25s - train_loss: 0.2002 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  258/387 151.89s - train_loss: 0.2002 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  259/387 152.52s - train_loss: 0.2002 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  260/387 153.16s - train_loss: 0.2003 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  261/387 153.80s - train_loss: 0.2003 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  262/387 154.46s - train_loss: 0.2002 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  263/387 155.15s - train_loss: 0.2002 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  264/387 155.85s - train_loss: 0.2002 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  265/387 156.52s - train_loss: 0.2001 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  266/387 157.18s - train_loss: 0.2001 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  267/387 157.83s - train_loss: 0.2000 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  268/387 158.47s - train_loss: 0.2000 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  269/387 159.10s - train_loss: 0.2000 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  270/387 159.73s - train_loss: 0.2000 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  271/387 160.37s - train_loss: 0.1999 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  272/387 161.04s - train_loss: 0.1999 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  273/387 161.73s - train_loss: 0.1999 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  274/387 162.40s - train_loss: 0.1999 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  275/387 163.08s - train_loss: 0.2000 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  276/387 163.74s - train_loss: 0.1999 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  277/387 164.38s - train_loss: 0.1999 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  278/387 165.02s - train_loss: 0.1998 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  279/387 165.65s - train_loss: 0.1998 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  280/387 166.28s - train_loss: 0.1998 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  281/387 166.94s - train_loss: 0.1998 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  282/387 167.62s - train_loss: 0.1998 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  283/387 168.31s - train_loss: 0.2000 train_acc: 0.9218 \n",
      "\u001B[1A\u001B[2K  284/387 168.98s - train_loss: 0.2002 train_acc: 0.9218 \n",
      "\u001B[1A\u001B[2K  285/387 169.64s - train_loss: 0.2001 train_acc: 0.9218 \n",
      "\u001B[1A\u001B[2K  286/387 170.29s - train_loss: 0.1999 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  287/387 170.93s - train_loss: 0.1999 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  288/387 171.56s - train_loss: 0.1999 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  289/387 172.19s - train_loss: 0.1998 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  290/387 172.82s - train_loss: 0.1998 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  291/387 173.48s - train_loss: 0.1998 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  292/387 174.16s - train_loss: 0.1998 train_acc: 0.9219 \n",
      "\u001B[1A\u001B[2K  293/387 174.85s - train_loss: 0.1998 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  294/387 175.53s - train_loss: 0.1997 train_acc: 0.9220 \n",
      "\u001B[1A\u001B[2K  295/387 176.19s - train_loss: 0.1996 train_acc: 0.9221 \n",
      "\u001B[1A\u001B[2K  296/387 176.84s - train_loss: 0.1996 train_acc: 0.9222 \n",
      "\u001B[1A\u001B[2K  297/387 177.48s - train_loss: 0.1994 train_acc: 0.9223 \n",
      "\u001B[1A\u001B[2K  298/387 178.11s - train_loss: 0.1995 train_acc: 0.9223 \n",
      "\u001B[1A\u001B[2K  299/387 178.74s - train_loss: 0.1993 train_acc: 0.9223 \n",
      "\u001B[1A\u001B[2K  300/387 179.39s - train_loss: 0.1992 train_acc: 0.9224 \n",
      "\u001B[1A\u001B[2K  301/387 180.06s - train_loss: 0.1992 train_acc: 0.9224 \n",
      "\u001B[1A\u001B[2K  302/387 180.76s - train_loss: 0.1994 train_acc: 0.9223 \n",
      "\u001B[1A\u001B[2K  303/387 181.44s - train_loss: 0.1994 train_acc: 0.9223 \n",
      "\u001B[1A\u001B[2K  304/387 182.11s - train_loss: 0.1993 train_acc: 0.9224 \n",
      "\u001B[1A\u001B[2K  305/387 182.77s - train_loss: 0.1993 train_acc: 0.9224 \n",
      "\u001B[1A\u001B[2K  306/387 183.41s - train_loss: 0.1993 train_acc: 0.9224 \n",
      "\u001B[1A\u001B[2K  307/387 184.05s - train_loss: 0.1993 train_acc: 0.9224 \n",
      "\u001B[1A\u001B[2K  308/387 184.69s - train_loss: 0.1991 train_acc: 0.9224 \n",
      "\u001B[1A\u001B[2K  309/387 185.32s - train_loss: 0.1990 train_acc: 0.9225 \n",
      "\u001B[1A\u001B[2K  310/387 185.98s - train_loss: 0.1989 train_acc: 0.9225 \n",
      "\u001B[1A\u001B[2K  311/387 186.67s - train_loss: 0.1988 train_acc: 0.9226 \n",
      "\u001B[1A\u001B[2K  312/387 187.36s - train_loss: 0.1986 train_acc: 0.9227 \n",
      "\u001B[1A\u001B[2K  313/387 188.03s - train_loss: 0.1986 train_acc: 0.9227 \n",
      "\u001B[1A\u001B[2K  314/387 188.69s - train_loss: 0.1985 train_acc: 0.9227 \n",
      "\u001B[1A\u001B[2K  315/387 189.33s - train_loss: 0.1985 train_acc: 0.9228 \n",
      "\u001B[1A\u001B[2K  316/387 189.96s - train_loss: 0.1986 train_acc: 0.9228 \n",
      "\u001B[1A\u001B[2K  317/387 190.59s - train_loss: 0.1986 train_acc: 0.9228 \n",
      "\u001B[1A\u001B[2K  318/387 191.22s - train_loss: 0.1986 train_acc: 0.9227 \n",
      "\u001B[1A\u001B[2K  319/387 191.86s - train_loss: 0.1985 train_acc: 0.9227 \n",
      "\u001B[1A\u001B[2K  320/387 192.52s - train_loss: 0.1984 train_acc: 0.9228 \n",
      "\u001B[1A\u001B[2K  321/387 193.21s - train_loss: 0.1983 train_acc: 0.9228 \n",
      "\u001B[1A\u001B[2K  322/387 193.88s - train_loss: 0.1982 train_acc: 0.9229 \n",
      "\u001B[1A\u001B[2K  323/387 194.55s - train_loss: 0.1983 train_acc: 0.9229 \n",
      "\u001B[1A\u001B[2K  324/387 195.20s - train_loss: 0.1982 train_acc: 0.9229 \n",
      "\u001B[1A\u001B[2K  325/387 195.85s - train_loss: 0.1982 train_acc: 0.9230 \n",
      "\u001B[1A\u001B[2K  326/387 196.48s - train_loss: 0.1981 train_acc: 0.9230 \n",
      "\u001B[1A\u001B[2K  327/387 197.11s - train_loss: 0.1980 train_acc: 0.9230 \n",
      "\u001B[1A\u001B[2K  328/387 197.75s - train_loss: 0.1980 train_acc: 0.9231 \n",
      "\u001B[1A\u001B[2K  329/387 198.40s - train_loss: 0.1981 train_acc: 0.9230 \n",
      "\u001B[1A\u001B[2K  330/387 199.08s - train_loss: 0.1981 train_acc: 0.9231 \n",
      "\u001B[1A\u001B[2K  331/387 199.77s - train_loss: 0.1980 train_acc: 0.9231 \n",
      "\u001B[1A\u001B[2K  332/387 200.44s - train_loss: 0.1981 train_acc: 0.9231 \n",
      "\u001B[1A\u001B[2K  333/387 201.10s - train_loss: 0.1981 train_acc: 0.9231 \n",
      "\u001B[1A\u001B[2K  334/387 201.76s - train_loss: 0.1981 train_acc: 0.9231 \n",
      "\u001B[1A\u001B[2K  335/387 202.41s - train_loss: 0.1980 train_acc: 0.9231 \n",
      "\u001B[1A\u001B[2K  336/387 203.07s - train_loss: 0.1980 train_acc: 0.9232 \n",
      "\u001B[1A\u001B[2K  337/387 203.72s - train_loss: 0.1979 train_acc: 0.9232 \n",
      "\u001B[1A\u001B[2K  338/387 204.37s - train_loss: 0.1978 train_acc: 0.9232 \n",
      "\u001B[1A\u001B[2K  339/387 205.03s - train_loss: 0.1978 train_acc: 0.9232 \n",
      "\u001B[1A\u001B[2K  340/387 205.70s - train_loss: 0.1978 train_acc: 0.9232 \n",
      "\u001B[1A\u001B[2K  341/387 206.36s - train_loss: 0.1978 train_acc: 0.9232 \n",
      "\u001B[1A\u001B[2K  342/387 207.00s - train_loss: 0.1978 train_acc: 0.9232 \n",
      "\u001B[1A\u001B[2K  343/387 207.64s - train_loss: 0.1978 train_acc: 0.9232 \n",
      "\u001B[1A\u001B[2K  344/387 208.26s - train_loss: 0.1976 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  345/387 208.89s - train_loss: 0.1977 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  346/387 209.50s - train_loss: 0.1976 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  347/387 210.12s - train_loss: 0.1976 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  348/387 210.76s - train_loss: 0.1976 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  349/387 211.42s - train_loss: 0.1976 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  350/387 212.10s - train_loss: 0.1975 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  351/387 212.78s - train_loss: 0.1976 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  352/387 213.44s - train_loss: 0.1975 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  353/387 214.09s - train_loss: 0.1975 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  354/387 214.73s - train_loss: 0.1975 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  355/387 215.36s - train_loss: 0.1975 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  356/387 215.98s - train_loss: 0.1974 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  357/387 216.60s - train_loss: 0.1976 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  358/387 217.24s - train_loss: 0.1975 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  359/387 217.91s - train_loss: 0.1975 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  360/387 218.58s - train_loss: 0.1975 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  361/387 219.26s - train_loss: 0.1976 train_acc: 0.9233 \n",
      "\u001B[1A\u001B[2K  362/387 219.92s - train_loss: 0.1975 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  363/387 220.56s - train_loss: 0.1974 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  364/387 221.19s - train_loss: 0.1973 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  365/387 221.82s - train_loss: 0.1972 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  366/387 222.45s - train_loss: 0.1972 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  367/387 223.07s - train_loss: 0.1972 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  368/387 223.72s - train_loss: 0.1973 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  369/387 224.39s - train_loss: 0.1973 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  370/387 225.07s - train_loss: 0.1972 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  371/387 225.73s - train_loss: 0.1972 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  372/387 226.38s - train_loss: 0.1971 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  373/387 227.03s - train_loss: 0.1971 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  374/387 227.65s - train_loss: 0.1971 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  375/387 228.28s - train_loss: 0.1971 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  376/387 228.90s - train_loss: 0.1971 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  377/387 229.52s - train_loss: 0.1971 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  378/387 230.17s - train_loss: 0.1971 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  379/387 230.84s - train_loss: 0.1971 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  380/387 231.51s - train_loss: 0.1972 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  381/387 232.15s - train_loss: 0.1972 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  382/387 232.77s - train_loss: 0.1972 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  383/387 233.35s - train_loss: 0.1973 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  384/387 233.91s - train_loss: 0.1972 train_acc: 0.9234 \n",
      "\u001B[1A\u001B[2K  385/387 234.45s - train_loss: 0.1971 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  386/387 234.98s - train_loss: 0.1971 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  387/387 235.38s - train_loss: 0.1972 train_acc: 0.9235 \n",
      "\u001B[1A\u001B[2K  283.11s - train_loss: 0.1972 train_acc: 0.9235 std_val_loss: 0.0004 std_val_acc: 0.9277 \n"
     ]
    },
    {
     "data": {
      "text/plain": "{'train_loss': 0.19716130146992608,\n 'train_acc': 0.9234583861470638,\n 'std_val_loss': 0.0003686105364637476,\n 'std_val_acc': 0.9276883919643669}"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(model, base_learning_rate, weight_decay, num_epochs=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Results and conclusion\n",
    "\n",
    "| model type  | learning rate | weight decay | number of filters per conv layers | number of parameters | validation accuracy |\n",
    "|-------------|---------------|--------------|-----------------------------------|----------------------|---------------------|\n",
    "| basic model | 0.001         | 0.0001       | 16, 2x16, 2x32, 2x64              | 75 010               | 90.63%              |\n",
    "| basic model | 0.001         | 0.0001       | 16, 2x32, 2x64, 2x128             | 292 386              | 91.28%              |\n",
    "| basic model | 0.001         | 0.0001       | 16, 2x16, 2x32, 2x64, 2x128       | 297 090              | 91.54%              |\n",
    "| resnet      | 0.001         | 0.0001       | 16, 4x16, 4x32, 4x64              | 174 546              | 91.62%              |\n",
    "| resnet      | 0.001         | 0.0001       | 16, 4x32, 4x64, 4x128             | 690 642              | 91.37%              |\n",
    "| resnet      | 0.001         | 0.0001       | 16, 4x16, 4x32, 4x64, 4x128       | 699 986              | 91.95%              |\n",
    "| resnet      | 0.0005        | 0.0001       | 16, 4x16, 4x32, 4x64, 4x128       | 699 986              | 90.76%              |\n",
    "| resnet      | 0.005         | 0.0001       | 16, 4x16, 4x32, 4x64, 4x128       | 699 986              | 92.69%              |\n",
    "| resnet      | 0.005         | 0.0005       | 16, 4x16, 4x32, 4x64, 4x128       | 699 986              | 92.77%              |\n",
    "\n",
    "(Some filters are writen like 4x16, to write it in a compact way, but it means that there are 4 different convolutional layers, all with 16 filters each.)\n",
    "\n",
    "Based on this little hyperparameter tuning, it seems that making the models deeper helps more than widening them by adding more filters to each convolutional layer. Also, it looks like doubling or tripling the number of parameters only adds 1-2% of accuracy to the final results, while it significantly increases the training time and the amount of memory used. It is probably possible to further increase the performance of the model by deepening the model with more additional convolutional layers, but I do not have the resources to run bigger models at the time of making this project."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "all_ids = []\n",
    "all_predictions = []\n",
    "model.eval()\n",
    "for images, ids in test_loader:\n",
    "    all_predictions += model(images.to(device)).argmax(dim=1).to(dtype=torch.int).tolist()\n",
    "    all_ids += ids\n",
    "submission = pd.DataFrame({'id': all_ids, 'target': all_predictions})\n",
    "submission.to_csv('submission.csv', sep=',', header=True, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
